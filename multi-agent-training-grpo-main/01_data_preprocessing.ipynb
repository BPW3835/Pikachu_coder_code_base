{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro-1",
   "metadata": {},
   "source": [
    "# End-to-End Dataset Preparation for LLM Fine-Tuning\n",
    "\n",
    "Welcome to this comprehensive guide on preparing a high-quality, mixed-domain dataset for fine-tuning a Large Language Model (LLM). The goal of fine-tuning is to adapt a pre-trained model to specific tasks or domains, enhancing its performance on them. The quality and structure of the fine-tuning dataset are paramount to achieving good results.\n",
    "\n",
    "In this notebook, we will perform a complete, end-to-end data preparation pipeline. We will source data from multiple distinct domains to create a robust training set and a challenging validation set. Specifically, we will:\n",
    "\n",
    "1.  **Source Datasets**: We will use three different datasets from the Hugging Face Hub:\n",
    "    *   `zwhe99/DeepMath-103K`: A large dataset of mathematical problems, designed to enhance a model's logical and mathematical reasoning capabilities.\n",
    "    *   `RUC-NLPIR/FlashRAG_datasets (Natural Questions)`: A widely-used open-domain question-answering dataset derived from real Google search queries. This adds general knowledge and conversational ability to our mix.\n",
    "    *   `Maxwell-Jia/AIME_2024`: A set of problems from the American Invitational Mathematics Examination (AIME). These are highly challenging, competition-level math problems, making them an excellent choice for a validation set to test the model's advanced reasoning skills on unseen, difficult tasks.\n",
    "\n",
    "2.  **Process and Standardize**: We will process each dataset, cleaning the data and transforming it into a single, unified format. This standardization is crucial for the model to learn from different data sources seamlessly.\n",
    "\n",
    "3.  **Combine and Shuffle**: We will combine the processed math and question-answering datasets into a single training file. We will then shuffle it to ensure the model sees a random mix of data types during training, which prevents it from learning any spurious order-based patterns.\n",
    "\n",
    "4.  **Prepare a Validation Set**: We will process the AIME dataset separately to serve as our validation set. This allows us to measure the model's performance on a task that is related but significantly more difficult than the training data.\n",
    "\n",
    "5.  **Save for Use**: Finally, we will save our curated training and validation sets in the efficient Parquet file format, ready to be used in an LLM fine-tuning pipeline.\n",
    "\n",
    "Let's begin!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup-1",
   "metadata": {},
   "source": [
    "## Section 1: Setup and Environment Preparation\n",
    "\n",
    "Before we can begin processing data, we need to set up our environment. This involves installing the necessary Python libraries, importing them into our script, and creating the directory structure where we will save our final datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup-2-install",
   "metadata": {},
   "source": [
    "### 1.1 Installing Dependencies\n",
    "\n",
    "We will use several key libraries. The following command ensures they are installed. If you are running this in a new environment, uncomment and run the cell below.\n",
    "\n",
    "*   `pandas` & `numpy`: Essential for data manipulation and numerical operations.\n",
    "*   `datasets`: The Hugging Face library for easily loading and manipulating large datasets.\n",
    "*   `ipywidgets` & `jupyter`: Required for rendering progress bars and interactive elements within the Jupyter environment.\n",
    "*   `tqdm`: A library for creating smart, simple progress bars for our loops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b011c794",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install pandas numpy datasets ipywidgets jupyter tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup-3-import",
   "metadata": {},
   "source": [
    "### 1.2 Importing Libraries\n",
    "\n",
    "Now, let's import all the modules we'll need for this notebook. We also add a line to filter out warnings to keep our output clean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c275cbde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports for interacting with the operating system and handling JSON data.\n",
    "import os\n",
    "import json\n",
    "\n",
    "# Core data science libraries for data manipulation and numerical computation.\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Hugging Face library for dataset loading and processing.\n",
    "from datasets import load_dataset, concatenate_datasets, Dataset\n",
    "\n",
    "# Utility for displaying progress bars, making long-running operations more informative.\n",
    "from tqdm import tqdm\n",
    "\n",
    "# This section is for suppressing ignorable warnings to keep the output tidy.\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup-4-dirs",
   "metadata": {},
   "source": [
    "### 1.3 Creating Output Directories\n",
    "\n",
    "A well-organized project structure is crucial. We will create dedicated directories for our training and validation data. This practice helps keep our project clean and makes it easy to locate our final artifacts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "831c67bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directories created: ./data/train, ./data/val\n"
     ]
    }
   ],
   "source": [
    "# Define the path for the training data output directory.\n",
    "train_output_dir = \"./data/train\"\n",
    "# Define the path for the validation data output directory.\n",
    "val_output_dir = \"./data/val\"\n",
    "\n",
    "# Use os.makedirs to create the directories. \n",
    "# The `exist_ok=True` argument prevents an error if the directories already exist.\n",
    "os.makedirs(train_output_dir, exist_ok=True)\n",
    "os.makedirs(val_output_dir, exist_ok=True)\n",
    "\n",
    "print(f\"Directories created: {train_output_dir}, {val_output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "train-1",
   "metadata": {},
   "source": [
    "## Section 2: Preparing the Training Dataset\n",
    "\n",
    "Our goal in this section is to create a single, high-quality training dataset by combining data from two different sources: mathematical problems and open-domain questions. This diversity will help the model become more versatile."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "train-2-math",
   "metadata": {},
   "source": [
    "### 2.1 The DeepMath-103K Dataset (Mathematical Reasoning)\n",
    "\n",
    "First, we'll process the `DeepMath-103K` dataset. This dataset contains over 100,000 math problems, complete with solutions. It's an excellent resource for teaching an LLM structured, step-by-step reasoning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "train-3-math-load",
   "metadata": {},
   "source": [
    "#### Loading the DeepMath Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8e3cbc7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Loading DeepMath-103K ===\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6367bd729943431fb9eab181ee0acf02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6971f3a0dc84fe5a12a0813b69ff310",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/2.05G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dab1f056e1f346f29900bd5d28b55ad4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b86c33d23f34932bd8a0c4c34a60dae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"\\n=== Loading DeepMath-103K ===\")\n",
    "\n",
    "# Use the `load_dataset` function from the `datasets` library.\n",
    "# We specify the dataset name on the Hugging Face Hub: \"zwhe99/DeepMath-103K\".\n",
    "# We also specify that we only want the \"train\" split of this dataset.\n",
    "math_dataset = load_dataset(\n",
    "    \"zwhe99/DeepMath-103K\",\n",
    "    split=\"train\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "train-4-math-explore",
   "metadata": {},
   "source": [
    "#### Exploring the DeepMath Dataset\n",
    "\n",
    "Before processing, it's always a good idea to inspect the dataset. We'll check its columns, the total number of samples, and look at one example record to understand its structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a580aa3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns: ['question', 'final_answer', 'difficulty', 'topic', 'r1_solution_1', 'r1_solution_2', 'r1_solution_3']\n",
      "Total samples: 103022\n"
     ]
    }
   ],
   "source": [
    "# The `.column_names` attribute gives us a list of all columns in the dataset.\n",
    "print(\"Columns:\", math_dataset.column_names)\n",
    "# The `len()` function tells us the total number of records (rows) in the dataset.\n",
    "print(\"Total samples:\", len(math_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "734c53d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"question\": \"Evaluate the limit: \\\\[ \\\\lim_{x \\\\to \\\\infty} \\\\sqrt{x} \\\\left( \\\\sqrt[3]{x+1} - \\\\sqrt[3]{x-1} \\\\right) \\\\]\",\n",
      "  \"final_answer\": \"0\",\n",
      "  \"difficulty\": 4.5,\n",
      "  \"topic\": \"Mathematics -> Precalculus -> Limits\",\n",
      "  \"r1_solution_1\": \"Okay, so I have this limit to evaluate: the limit as x approaches infinity of the square root of x times the difference between the cube root of (x plus 1) and the cube root of (x minus 1). Hmm, let me write that down again to make sure I have it right.\\n\\n\\\\[\\n\\\\lim_{x \\\\to \\\\infty} \\\\sqrt{x} \\\\left( \\\\sqrt[3]{x+1} - \\\\sqrt[3]{x-1} \\\\right)\\n\\\\]\\n\\nAlright, so it's the product of sqrt(x) and the difference of tw\",\n",
      "  \"r1_solution_2\": \"Okay, so I need to evaluate the limit as x approaches infinity of sqrt(x) times (the cube root of (x+1) minus the cube root of (x-1)). Let me write that down again to make sure I got it right:\\n\\n\\\\[\\n\\\\lim_{x \\\\to \\\\infty} \\\\sqrt{x} \\\\left( \\\\sqrt[3]{x+1} - \\\\sqrt[3]{x-1} \\\\right)\\n\\\\]\\n\\nHmm. So the expression is sqrt(x) multiplied by the difference of two cube roots. Both cube roots are of expressions that are\",\n",
      "  \"r1_solution_3\": \"Okay, so I need to evaluate the limit as x approaches infinity of sqrt(x) times (the cube root of (x+1) minus the cube root of (x-1)). Hmm, that looks a bit intimidating at first, but maybe I can break it down step by step. Let me write it out again to visualize better:\\n\\n\\\\[\\n\\\\lim_{x \\\\to \\\\infty} \\\\sqrt{x} \\\\left( \\\\sqrt[3]{x+1} - \\\\sqrt[3]{x-1} \\\\right)\\n\\\\]\\n\\nFirst, let's recall that when dealing with limi\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Accessing an item by index, like a list, gives us a single record.\n",
    "sample = math_dataset[0]\n",
    "\n",
    "# The solution fields ('r1_solution_*') can be very long. \n",
    "# For a clean printout, we'll truncate them.\n",
    "truncated_sample = sample.copy()\n",
    "for key in ['r1_solution_1', 'r1_solution_2', 'r1_solution_3']:\n",
    "    truncated_sample[key] = sample[key][:400]\n",
    "\n",
    "# Use `json.dumps` with indentation for a pretty, readable print of the sample record.\n",
    "print(json.dumps(truncated_sample, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "train-5-math-process",
   "metadata": {},
   "source": [
    "#### Standardizing the DeepMath Dataset\n",
    "\n",
    "Now we'll iterate through each record and convert it to our desired standard format. This format is generic and will allow us to combine it with other datasets later. \n",
    "\n",
    "Our target schema will be:\n",
    "*   `id`: A unique identifier for each sample.\n",
    "*   `question`: The problem or query text.\n",
    "*   `chain`: A placeholder for chain-of-thought or reasoning steps (we'll leave it empty for now).\n",
    "*   `result`: The final answer.\n",
    "*   `source`: A string indicating the original dataset.\n",
    "*   `extra_info`: A dictionary to hold any other useful metadata from the original record."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "38d80772",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Processing MathHard ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing MathHard: 100%|██████████| 103022/103022 [00:03<00:00, 33261.05it/s]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== Processing MathHard ===\")\n",
    "\n",
    "# Initialize an empty list to store our processed records.\n",
    "math_rows = []\n",
    "\n",
    "# We iterate through the dataset using tqdm to get a nice progress bar.\n",
    "# `enumerate` gives us both the index (`idx`) and the item for each record.\n",
    "for idx, item in enumerate(tqdm(math_dataset, desc=\"Processing MathHard\")):\n",
    "    # Some datasets might use different keys for the same concept (e.g., 'question' vs 'Problem').\n",
    "    # This logic handles such inconsistencies gracefully.\n",
    "    if \"question\" in item:\n",
    "        question = item[\"question\"]\n",
    "    elif \"Problem\" in item:\n",
    "        question = item[\"Problem\"]\n",
    "    else:\n",
    "        # If neither key is found, raise an error to stop execution, as this is unexpected.\n",
    "        raise KeyError(\"Missing question field\")\n",
    "\n",
    "    # Similarly, handle potential inconsistencies for the answer field.\n",
    "    if \"final_answer\" in item:\n",
    "        answer = item[\"final_answer\"]\n",
    "    elif \"Answer\" in item:\n",
    "        answer = item[\"Answer\"]\n",
    "    else:\n",
    "        raise KeyError(\"Missing answer field\")\n",
    "\n",
    "    # Append a new dictionary to our list, structured according to our standard format.\n",
    "    math_rows.append({\n",
    "        \"id\": idx,  # Use the loop index as a temporary ID.\n",
    "        \"question\": question,\n",
    "        \"chain\": \"\",  # Placeholder for reasoning steps.\n",
    "        \"result\": str(answer), # Ensure the answer is always a string.\n",
    "        \"source\": \"mathhard\", # Tag the data source.\n",
    "        \"extra_info\": { # Store original metadata.\n",
    "            \"ground_truth\": str(answer),\n",
    "            \"idx\": idx\n",
    "        }\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c8108fb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed math samples: 103022\n",
      "\n",
      "Processed sample:\n",
      "{\n",
      "  \"id\": 0,\n",
      "  \"question\": \"Evaluate the limit: \\\\[ \\\\lim_{x \\\\to \\\\infty} \\\\sqrt{x} \\\\left( \\\\sqrt[3]{x+1} - \\\\sqrt[3]{x-1} \\\\right) \\\\]\",\n",
      "  \"chain\": \"\",\n",
      "  \"result\": \"0\",\n",
      "  \"source\": \"mathhard\",\n",
      "  \"extra_info\": {\n",
      "    \"ground_truth\": \"0\",\n",
      "    \"idx\": 0\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Verify that the number of processed rows matches the original dataset size.\n",
    "print(\"Processed math samples:\", len(math_rows))\n",
    "print(\"\\nProcessed sample:\")\n",
    "# Print the first processed sample to confirm it matches our target format.\n",
    "print(json.dumps(math_rows[0], indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "train-6-math-convert",
   "metadata": {},
   "source": [
    "#### Converting to a `Dataset` Object\n",
    "\n",
    "Our processed data is currently a Python list of dictionaries. For better performance and compatibility with the Hugging Face ecosystem (like the `Trainer` API), we'll convert it into a `datasets.Dataset` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4bdb8788",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, convert the list of dictionaries into a pandas DataFrame.\n",
    "# Then, use `Dataset.from_pandas` to create the Hugging Face Dataset object.\n",
    "# `preserve_index=False` tells the function not to add the DataFrame's index as a new column.\n",
    "ds_math = Dataset.from_pandas(\n",
    "    pd.DataFrame(math_rows),\n",
    "    preserve_index=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "train-7-nq",
   "metadata": {},
   "source": [
    "### 2.2 The Natural Questions (NQ) Dataset (Open-Domain QA)\n",
    "\n",
    "Now we'll repeat the process for the Natural Questions dataset. This dataset consists of real user questions posed to Google Search and their corresponding answers found on Wikipedia. Adding this data helps the model with general knowledge and fact-based queries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "train-8-nq-load",
   "metadata": {},
   "source": [
    "#### Loading the NQ Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "39bed48c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Loading FlashRAG NQ ===\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "838f17d3f0e24626978b68b827c01b44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/6.10k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9aa3bfa7ff8e4a8ba91d47b152dc55cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/10.1k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25125f180b5346519c3b62c5af143564",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ffd56445dd1a4c2ab80d85baf8417ecf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/9.96M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e9d957b6f3b4b09bb3bde83aac73c5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/1.07M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "636cbd1d4ea643659d700f451b7761cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/1.07M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Generating dev split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"\\n=== Loading FlashRAG NQ ===\")\n",
    "\n",
    "# `load_dataset` can take multiple arguments.\n",
    "# The first is the dataset group, \"RUC-NLPIR/FlashRAG_datasets\".\n",
    "# The second is the specific dataset name within that group, \"nq\".\n",
    "nq_dataset = load_dataset(\n",
    "    \"RUC-NLPIR/FlashRAG_datasets\",\n",
    "    \"nq\",\n",
    "    split=\"train\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "train-9-nq-explore",
   "metadata": {},
   "source": [
    "#### Exploring the NQ Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9b20c87c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns: ['id', 'question', 'golden_answers']\n",
      "Total samples: 79168\n"
     ]
    }
   ],
   "source": [
    "# Inspect the NQ dataset's structure.\n",
    "print(\"Columns:\", nq_dataset.column_names)\n",
    "print(\"Total samples:\", len(nq_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4a805386",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Raw NQ sample:\n",
      "{\n",
      "  \"id\": \"train_0\",\n",
      "  \"question\": \"total number of death row inmates in the us\",\n",
      "  \"golden_answers\": [\n",
      "    \"2,718\"\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Look at the first sample to understand the data format.\n",
    "print(\"\\nRaw NQ sample:\")\n",
    "print(json.dumps(nq_dataset[0], indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "train-10-nq-process",
   "metadata": {},
   "source": [
    "#### Standardizing the NQ Dataset\n",
    "\n",
    "The processing for NQ is slightly more complex. We need to perform some cleaning:\n",
    "\n",
    "1.  **Format Question**: Ensure every question ends with a question mark for consistency.\n",
    "2.  **Handle Answer Types**: The `golden_answers` field can contain data in various formats (lists, numpy arrays, strings, etc.). Our code needs to robustly handle all these cases, extract the answer(s), and convert them to a single string.\n",
    "3.  **Join Multiple Answers**: Some questions might have multiple valid answers. We will join them together into a single string, separated by a semicolon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fd50a3c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Processing NQ ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing NQ: 100%|██████████| 79168/79168 [00:01<00:00, 48264.44it/s]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== Processing NQ ===\")\n",
    "\n",
    "# Initialize an empty list to store processed NQ records.\n",
    "nq_rows = []\n",
    "\n",
    "# Iterate through the NQ dataset with a progress bar.\n",
    "for idx, item in enumerate(tqdm(nq_dataset, desc=\"Processing NQ\")):\n",
    "    # Get the question, remove leading/trailing whitespace.\n",
    "    question = item.get(\"question\", \"\").strip()\n",
    "    # Ensure the question ends with a '?' for consistency.\n",
    "    if question and not question.endswith(\"?\"):\n",
    "        question += \"?\"\n",
    "\n",
    "    # Get the answers, defaulting to an empty list if not present.\n",
    "    golden_answers = item.get(\"golden_answers\", [])\n",
    "    cleaned_answers = [] # This list will hold valid, string-formatted answers.\n",
    "\n",
    "    # The following block robustly handles various data types for the answers.\n",
    "    if isinstance(golden_answers, np.ndarray):\n",
    "        for x in golden_answers.flatten(): # Flatten in case of multi-dimensional array.\n",
    "            if x is not None and pd.notna(x):\n",
    "                cleaned_answers.append(str(x))\n",
    "    elif isinstance(golden_answers, (list, tuple)):\n",
    "        for x in golden_answers:\n",
    "            if x is not None and pd.notna(x):\n",
    "                cleaned_answers.append(str(x))\n",
    "    elif isinstance(golden_answers, str):\n",
    "        if golden_answers.strip():\n",
    "            cleaned_answers.append(golden_answers.strip())\n",
    "    elif isinstance(golden_answers, (int, float, np.generic)):\n",
    "        if not pd.isna(golden_answers):\n",
    "            cleaned_answers.append(str(golden_answers))\n",
    "    else: # Catch-all for any other types.\n",
    "        s = str(golden_answers).strip()\n",
    "        if s and s != \"nan\": # Avoid adding 'nan' as an answer.\n",
    "            cleaned_answers.append(s)\n",
    "\n",
    "    # Join all cleaned answers into a single string, separated by \"; \".\n",
    "    final_result = \"; \".join(cleaned_answers)\n",
    "\n",
    "    # Append the record in our standard format.\n",
    "    nq_rows.append({\n",
    "        \"id\": idx,  # Temporary ID.\n",
    "        \"question\": question,\n",
    "        \"chain\": \"\",\n",
    "        \"result\": final_result,\n",
    "        \"source\": \"nq\", # Tag the source as Natural Questions.\n",
    "        \"extra_info\": {\n",
    "            \"ground_truth\": final_result,\n",
    "            \"idx\": idx\n",
    "        }\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aa3ac0fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed NQ samples: 79168\n",
      "\n",
      "Processed NQ sample:\n",
      "{\n",
      "  \"id\": 0,\n",
      "  \"question\": \"total number of death row inmates in the us?\",\n",
      "  \"chain\": \"\",\n",
      "  \"result\": \"2,718\",\n",
      "  \"source\": \"nq\",\n",
      "  \"extra_info\": {\n",
      "    \"ground_truth\": \"2,718\",\n",
      "    \"idx\": 0\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Verify the number of processed samples and check the first record.\n",
    "print(\"Processed NQ samples:\", len(nq_rows))\n",
    "print(\"\\nProcessed NQ sample:\")\n",
    "print(json.dumps(nq_rows[0], indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "train-11-nq-convert",
   "metadata": {},
   "source": [
    "#### Converting to a `Dataset` Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9bc7755c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the processed NQ data into a Hugging Face Dataset object.\n",
    "ds_nq = Dataset.from_pandas(\n",
    "    pd.DataFrame(nq_rows),\n",
    "    preserve_index=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "train-12-combine",
   "metadata": {},
   "source": [
    "### 2.3 Combining and Finalizing the Training Set\n",
    "\n",
    "With both datasets processed and standardized, the final step is to merge them into a single training set. We will then shuffle this combined dataset and assign new, unique IDs to each record."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "train-13-combine-concat",
   "metadata": {},
   "source": [
    "#### Concatenating Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "124af40b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Combining datasets ===\n",
      "Combined size: 182190\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== Combining datasets ===\")\n",
    "# `concatenate_datasets` takes a list of Dataset objects and merges them row-wise.\n",
    "combined = concatenate_datasets([ds_nq, ds_math])\n",
    "print(\"Combined size:\", len(combined))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "train-14-combine-shuffle",
   "metadata": {},
   "source": [
    "#### Shuffling and Re-indexing\n",
    "\n",
    "**Shuffling** is a critical step. If we don't shuffle, the model will first see all 79,168 NQ samples, and then all 103,022 math samples. This can bias the learning process. By shuffling, we ensure that each training batch contains a random mix of data types, leading to more robust learning.\n",
    "\n",
    "**Re-indexing** is necessary because after combining and shuffling, the original `id`s are no longer unique or sequential. We apply a mapping function to assign a new, clean, sequential ID from 0 to N-1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0c77f5ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Shuffling ===\n",
      "\n",
      "=== Re-indexing IDs ===\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05159cd8036841a2872cca28037eeb5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Shuffle:   0%|          | 0/182190 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05159cd8036841a2872cca28037eeb5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/182190 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"\\n=== Shuffling ===\")\n",
    "# The `.shuffle()` method randomizes the order of the rows in the dataset.\n",
    "# Providing a `seed` ensures that the shuffle is reproducible. Anyone running this code\n",
    "# with the same seed will get the exact same shuffled order.\n",
    "combined = combined.shuffle(seed=42)\n",
    "\n",
    "print(\"\\n=== Re-indexing IDs ===\")\n",
    "# The `.map()` method applies a function to each element of the dataset.\n",
    "# Here, we use a lambda function that ignores the sample (`_`) and uses the index (`idx`).\n",
    "# `with_indices=True` provides the index of each row to our function.\n",
    "# This effectively replaces the old 'id' column with a new one from 0 to len-1.\n",
    "combined = combined.map(\n",
    "    lambda _, idx: {\"id\": idx},\n",
    "    with_indices=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2440008b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final combined sample:\n",
      "{\n",
      "  \"id\": 0,\n",
      "  \"question\": \"he classical string quartet is a musical composition for?\",\n",
      "  \"chain\": \"\",\n",
      "  \"result\": \"a viola player; a cellist; two violin players\",\n",
      "  \"source\": \"nq\",\n",
      "  \"extra_info\": {\n",
      "    \"ground_truth\": \"a viola player; a cellist; two violin players\",\n",
      "    \"idx\": 36700\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Let's inspect the very first sample of our final combined and shuffled dataset.\n",
    "# Note the new `id` is 0, and the `source` is 'nq', showing the shuffle worked.\n",
    "# The `idx` inside `extra_info` still refers to its original index in the NQ dataset.\n",
    "print(\"\\nFinal combined sample:\")\n",
    "print(json.dumps(combined[0], indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "train-15-combine-save",
   "metadata": {},
   "source": [
    "#### Saving the Training Dataset\n",
    "\n",
    "Finally, we save our completed training dataset to a file. We use the Parquet format, which is a highly efficient, column-oriented data format ideal for large datasets. It's widely supported and generally faster to read than formats like CSV or JSON."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "535f1302",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb6b5cb6008a42cdbb511fd82ec10325",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/19 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Train dataset saved to ./data/train\\combined_train.parquet\n"
     ]
    }
   ],
   "source": [
    "# Construct the full output file path using the directory we defined earlier.\n",
    "output_path = os.path.join(train_output_dir, \"combined_train.parquet\")\n",
    "# Use the `.to_parquet()` method to save the dataset.\n",
    "combined.to_parquet(output_path)\n",
    "\n",
    "print(f\"\\n✅ Train dataset saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "val-1",
   "metadata": {},
   "source": [
    "## Section 3: Preparing the Validation Dataset\n",
    "\n",
    "A good validation set should be representative of the tasks we care about, but it should not overlap with the training data. For this purpose, we will use a small, high-quality, and challenging dataset: problems from the AIME 2024 competition. This will allow us to rigorously test our model's advanced reasoning abilities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "val-2-aime",
   "metadata": {},
   "source": [
    "### 3.1 The AIME 2024 Dataset (Advanced Math Competition)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "val-3-aime-load",
   "metadata": {},
   "source": [
    "#### Loading and Exploring the AIME Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f8806a31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Loading AIME 2024 ===\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f8940b4e16144e3876a638adf9b4589",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/4.44k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efc3a8668f9444e5be68512ff2555034",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/1.03k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2a5ac07c51e40588f69849f15748473",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/37.2k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Generating train split: 30 examples [00:00, 192.65 examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"\\n=== Loading AIME 2024 ===\")\n",
    "\n",
    "# Load the AIME dataset from the Hugging Face Hub.\n",
    "aime_dataset = load_dataset(\n",
    "    \"Maxwell-Jia/AIME_2024\",\n",
    "    split=\"train\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "92498859",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns: ['ID', 'Problem', 'Solution', 'Answer']\n",
      "Total samples: 30\n"
     ]
    }
   ],
   "source": [
    "# Check the columns and size of this small but challenging dataset.\n",
    "print(\"Columns:\", aime_dataset.column_names)\n",
    "print(\"Total samples:\", len(aime_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "093b7ac4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Raw sample:\n",
      "{\n",
      "  \"ID\": \"2024-II-4\",\n",
      "  \"Problem\": \"Let $x,y$ and $z$ be positive real numbers that satisfy the following system of equations: \\n\\\\[\\\\log_2\\\\left({x \\\\over yz}\\\\right) = {1 \\\\over 2}\\\\]\\n\\\\[\\\\log_2\\\\left({y \\\\over xz}\\\\right) = {1 \\\\over 3}\\\\]\\n\\\\[\\\\log_2\\\\left({z \\\\over xy}\\\\right) = {1 \\\\over 4}\\\\]\\nThen the value of $\\\\left|\\\\log_2(x^4y^3z^2)\\\\right|$ is $\\\\tfrac{m}{n}$ where $m$ and $n$ are relatively prime positive integers. Find $m+n$.\",\n",
      "  \"Solution\": \"Denote $\\\\log_2(x) = a$, $\\\\log_2(y) = b$, and $\\\\log_2(z) = c$.\\n\\nThen, we have:\\n$a-b-c = \\\\frac{1}{2}$,\\n$-a+b-c = \\\\frac{1}{3}$,\\n$-a-b+c = \\\\frac{1}{4}$.\\n\\nNow, we can solve to get $a = \\\\frac{-7}{24}, b = \\\\frac{-9}{24}, c = \\\\frac{-5}{12}$.\\nPlugging these values in, we obtain $|4a + 3b + 2c|  = \\\\frac{25}{8} \\\\implies \\\\boxed{033}$.\",\n",
      "  \"Answer\": 33\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Examine a sample record to see the structure.\n",
    "print(\"\\nRaw sample:\")\n",
    "print(json.dumps(aime_dataset[0], indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "val-4-aime-process",
   "metadata": {},
   "source": [
    "#### Processing the AIME Dataset for Evaluation\n",
    "\n",
    "We will process this dataset into our standard format, but with one important modification. For evaluation, it's very helpful if the model outputs its final answer in a predictable, easy-to-parse format. \n",
    "\n",
    "To achieve this, we will add a specific instruction to the end of each question prompt, telling the model to enclose its final answer in `<answer>` and `</answer>` tags. This is a common prompt engineering technique for improving the reliability of automated evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "67d83fda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Processing AIME 2024 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing AIME 2024: 100%|██████████| 30/30 [00:00<00:00, 31924.96it/s]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== Processing AIME 2024 ===\")\n",
    "\n",
    "# This is the instruction string we will append to each question.\n",
    "OUTPUT_FORMAT = (\n",
    "    \"When ready, output the final answer enclosed in <answer> and </answer> tags. \"\n",
    "    \"Do not generate any content after the </answer> tag.\"\n",
    ")\n",
    "\n",
    "# Initialize an empty list for the processed validation data.\n",
    "aime_rows = []\n",
    "\n",
    "# Iterate through the AIME dataset.\n",
    "for idx, item in enumerate(tqdm(aime_dataset, desc=\"Processing AIME 2024\")):\n",
    "    # Extract the problem and answer, cleaning up whitespace.\n",
    "    problem = item.get(\"Problem\", \"\").strip()\n",
    "    answer = item.get(\"Answer\", \"\")\n",
    "\n",
    "    # Construct the full question by combining the original problem with our instruction string.\n",
    "    full_question = (\n",
    "        f\"{problem}\\n\\n{OUTPUT_FORMAT}\"\n",
    "        if problem else OUTPUT_FORMAT\n",
    "    )\n",
    "\n",
    "    # Append the record in our standard format.\n",
    "    aime_rows.append({\n",
    "        \"id\": idx,\n",
    "        \"question\": full_question,\n",
    "        \"chain\": \"\", # Left empty as this is for evaluation input.\n",
    "        \"result\": str(answer).strip(),\n",
    "        \"source\": \"aime2024\",\n",
    "        \"extra_info\": {\n",
    "            \"ground_truth\": str(answer).strip(),\n",
    "            \"idx\": idx,\n",
    "            \"original_problem\": problem # Store the original problem text for reference.\n",
    "        }\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "da434c6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed AIME samples: 30\n",
      "\n",
      "Processed sample:\n",
      "{\n",
      "  \"id\": 0,\n",
      "  \"question\": \"Let $x,y$ and $z$ be positive real numbers that satisfy the following system of equations: \\n\\\\[\\\\log_2\\\\left({x \\\\over yz}\\\\right) = {1 \\\\over 2}\\\\]\\n\\\\[\\\\log_2\\\\left({y \\\\over xz}\\\\right) = {1 \\\\over 3}\\\\]\\n\\\\[\\\\log_2\\\\left({z \\\\over xy}\\\\right) = {1 \\\\over 4}\\\\]\\nThen the value of $\\\\left|\\\\log_2(x^4y^3z^2)\\\\right|$ is $\\\\tfrac{m}{n}$ where $m$ and $n$ are relatively prime positive integers. Find $m+n$.\\n\\nWhen ready, output the final answer enclosed in <answer> and </answer> tags. Do not generate any content after the </answer> tag.\",\n",
      "  \"chain\": \"\",\n",
      "  \"result\": \"33\",\n",
      "  \"source\": \"aime2024\",\n",
      "  \"extra_info\": {\n",
      "    \"ground_truth\": \"33\",\n",
      "    \"idx\": 0,\n",
      "    \"original_problem\": \"Let $x,y$ and $z$ be positive real numbers that satisfy the following system of equations: \\n\\\\[\\\\log_2\\\\left({x \\\\over yz}\\\\right) = {1 \\\\over 2}\\\\]\\n\\\\[\\\\log_2\\\\left({y \\\\over xz}\\\\right) = {1 \\\\over 3}\\\\]\\n\\\\[\\\\log_2\\\\left({z \\\\over xy}\\\\right) = {1 \\\\over 4}\\\\]\\nThen the value of $\\\\left|\\\\log_2(x^4y^3z^2)\\\\right|$ is $\\\\tfrac{m}{n}$ where $m$ and $n$ are relatively prime positive integers. Find $m+n$.\"\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Verify the processed count and check the first sample to see the appended instruction.\n",
    "print(\"Processed AIME samples:\", len(aime_rows))\n",
    "print(\"\\nProcessed sample:\")\n",
    "print(json.dumps(aime_rows[0], indent=2))\n",
    "\n",
    "# Convert the list of dictionaries into a Dataset object.\n",
    "ds_aime = Dataset.from_pandas(\n",
    "    pd.DataFrame(aime_rows),\n",
    "    preserve_index=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "val-5-aime-finalize",
   "metadata": {},
   "source": [
    "#### Finalizing the Validation Set\n",
    "\n",
    "Just as we did with the training data, we will shuffle and re-index the validation set. While shuffling is less critical for a validation set (since it's not used for gradient updates), it's good practice for consistency and to avoid any potential ordering bias during evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "07d43a2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Shuffling validation data ===\n",
      "Re-indexing IDs...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d20f39b83855469badd6869a4bfd4ba7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Shuffle:   0%|          | 0/30 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d20f39b83855469badd6869a4bfd4ba7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/30 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"\\n=== Shuffling validation data ===\")\n",
    "# Shuffle the validation set with the same seed for reproducibility.\n",
    "ds_aime = ds_aime.shuffle(seed=42)\n",
    "\n",
    "print(\"Re-indexing IDs...\")\n",
    "# Re-assign sequential IDs to the shuffled validation data.\n",
    "ds_aime = ds_aime.map(\n",
    "    lambda _, idx: {\"id\": idx},\n",
    "    with_indices=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "val-6-aime-save",
   "metadata": {},
   "source": [
    "#### Saving the Validation Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6e34d63e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22f23c025f1146ffa990ec27eb524d80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Construct the full path for the validation file.\n",
    "val_path = os.path.join(val_output_dir, \"aime24.parquet\")\n",
    "# Save the final validation dataset to a Parquet file.\n",
    "ds_aime.to_parquet(val_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "75f17680",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Validation dataset saved to ./data/val\\aime24.parquet\n",
      "\n",
      "Validation prompt sample:\n",
      "\n",
      "Let $\\omega \\neq 1$ be a 13th root of unity. Find the remainder when \n",
      "\\[ \\prod_{k=0}^{12}(2 - 2\\omega^k + \\omega^{2k}) \\] is divided by 1000.\n",
      "\n",
      "When ready, output the final answer enclosed in <answer> and </answer> tags. Do not generate any content after the </answer> tag.\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n✅ Validation dataset saved to {val_path}\")\n",
    "# Print a sample prompt from the final validation set to confirm our formatting.\n",
    "print(\"\\nValidation prompt sample:\\n\")\n",
    "print(ds_aime[0][\"question\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "final-1",
   "metadata": {},
   "source": [
    "## Section 4: Final Checks and Conclusion\n",
    "\n",
    "We have successfully created our training and validation datasets. As a final step, let's perform some sanity checks to verify the size and sample counts of our output files."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "final-2-check-size",
   "metadata": {},
   "source": [
    "### 4.1 Checking File Sizes\n",
    "\n",
    "We'll check the on-disk size of the generated Parquet files. This is useful for understanding storage requirements and for a high-level confirmation that the files were written correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3ebca41e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train parquet size: 18.92 MB\n",
      "Validation parquet size: 0.02 MB\n"
     ]
    }
   ],
   "source": [
    "# Get the file path for the training parquet file.\n",
    "train_path = os.path.join(train_output_dir, \"combined_train.parquet\")\n",
    "# Get the file path for the validation parquet file.\n",
    "val_path = os.path.join(val_output_dir, \"aime24.parquet\")\n",
    "\n",
    "# Use os.path.getsize to get the file size in bytes.\n",
    "train_size = os.path.getsize(train_path)\n",
    "val_size = os.path.getsize(val_path)\n",
    "\n",
    "# Convert bytes to megabytes (MB) for easier reading and print the results.\n",
    "print(f\"\\nTrain parquet size: {train_size / (1024 * 1024):.2f} MB\")\n",
    "print(f\"Validation parquet size: {val_size / (1024 * 1024):.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "final-3-check-count",
   "metadata": {},
   "source": [
    "### 4.2 Checking Sample Counts\n",
    "\n",
    "Let's verify the total number of records in each of our final datasets. This should match the counts we saw during processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1818519b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total train samples: 182190\n",
      "Total validation samples: 30\n"
     ]
    }
   ],
   "source": [
    "# The length of our in-memory Dataset objects gives the total number of samples.\n",
    "train_count = len(combined)\n",
    "print(f\"\\nTotal train samples: {train_count}\")\n",
    "val_count = len(ds_aime)\n",
    "print(f\"Total validation samples: {val_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "final-4-conclusion",
   "metadata": {},
   "source": [
    "### 4.3 Conclusion\n",
    "\n",
    "In this notebook, we have successfully executed a full data preparation pipeline. We started with raw datasets from different domains, processed and standardized them, combined them into a robust training set, and created a challenging validation set.\n",
    "\n",
    "Our final artifacts are:\n",
    "*   `./data/train/combined_train.parquet`: A shuffled training set with 182,190 samples, mixing mathematical reasoning with open-domain question answering.\n",
    "*   `./data/val/aime24.parquet`: A high-difficulty validation set with 30 competition-level math problems, formatted for easy evaluation.\n",
    "\n",
    "These datasets are now ready to be used as inputs for fine-tuning a Large Language Model to enhance its reasoning and problem-solving capabilities."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}