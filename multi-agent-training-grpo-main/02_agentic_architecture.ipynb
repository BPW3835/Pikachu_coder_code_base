{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro-1",
   "metadata": {},
   "source": [
    "# 02. Agentic Workflow: An In-Depth Codebase Recreation\n",
    "\n",
    "Welcome to this detailed, educational walkthrough of a complete agentic workflow. This notebook is not just a demonstration but a faithful, step-by-step reconstruction of the core `multi-agent` inference engine. We will explore the exact, full-length prompts and the complete logic for each tool and component.\n",
    "\n",
    "## What is an Multi-Agent System?\n",
    "\n",
    "An agentic workflow, or a multi-agent system, is a sophisticated framework where a problem is solved by a series of specialized components (or \"agents\") that collaborate. Instead of relying on a single, monolithic LLM call to solve a complex query, this approach breaks the problem down into manageable stages:\n",
    "\n",
    "1.  **Planning:** An agent analyzes the initial query and formulates a high-level plan or decides on the immediate next best action.\n",
    "2.  **Tool Use:** The agent selects and uses specialized tools (like a code interpreter, a web search, or a database query tool) to gather information or perform actions.\n",
    "3.  **Execution:** A dedicated component generates the precise command to run the selected tool.\n",
    "4.  **Observation & Reflection:** The agent observes the result of the tool's execution and reflects on whether the goal has been achieved or if more steps are needed.\n",
    "5.  **Iteration:** The process repeats in a loop until a verifier agent determines that the query has been fully answered.\n",
    "6.  **Synthesis:** Finally, an agent synthesizes all the information gathered throughout the process into a comprehensive final answer.\n",
    "\n",
    "This notebook will build each of these components from the ground up, providing a clear and powerful learning experience.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intro-prereq",
   "metadata": {},
   "source": [
    "## Prerequisite: Start the vLLM Server\n",
    "\n",
    "This entire workflow is powered by a Large Language Model. To run this notebook, you must first have a vLLM server running. vLLM is a high-throughput serving engine for LLMs that provides an OpenAI-compatible API endpoint.\n",
    "\n",
    "Open a separate terminal and run the following command. You can replace the model with any other compatible model, but ensure the `MODEL_NAME` variable in the configuration cell below matches.\n",
    "\n",
    "```bash\n",
    "# Note: The model name in the command must match the MODEL_NAME variable in the notebook.\n",
    "# Start the vLLM server with the same model name as MODEL_NAME below\n",
    "# This can be a fine-tuned model or a base model, but it must match the name used in the notebook configuration.\n",
    "vllm serve qwen-1.5-finetuned-grpo \\ \n",
    "    --api-key a-secret-key \\        # API key for OpenAI-compatible auth\n",
    "    --port 8000 \\                   # Port for the local server\n",
    "    --max-model-len 8192            # Maximum context length\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup-1",
   "metadata": {},
   "source": [
    "## Section 1: Setup, Dependencies, and Configuration\n",
    "\n",
    "We begin by setting up our environment. This involves installing the required Python libraries, importing necessary modules, and configuring the connection to our vLLM server and other external services."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "296c562b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell lists the required libraries for the project. \n",
    "# Uncomment the line below and run it if you haven't installed these packages yet.\n",
    "# openai: The client library for interacting with OpenAI-compatible APIs (like our vLLM server).\n",
    "# pydantic: For data validation and creating structured data models, crucial for reliable agent outputs.\n",
    "# tenacity: A library for adding retry logic to functions, making our API calls more robust.\n",
    "# beautifulsoup4 & requests: For fetching and parsing HTML content from websites (used in the Web_Search_Tool).\n",
    "# wikipedia: A convenient wrapper for the Wikipedia API.\n",
    "# google-genai: The client library for Google's Generative AI services (used in the Google_Search_Tool).\n",
    "# numpy: For numerical operations, specifically for calculating vector similarity in our RAG tool.\n",
    "# json_repair: A helpful utility to fix malformed JSON strings, which LLMs sometimes produce.\n",
    "\n",
    "# !pip install -q openai==1.75.0 pydantic==2.5.3 tenacity==9.0.0 beautifulsoup4==4.12.3 requests==2.31.0 wikipedia==1.4.0 google-genai==0.8.5 numpy==1.26.4 json_repair"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup-2",
   "metadata": {},
   "source": [
    "### 1.1 Imports and Global Configuration\n",
    "\n",
    "Here, we import all necessary modules and define the core configuration variables. This includes the vLLM server URL, the model name, and placeholders for API keys required by our tools. \n",
    "\n",
    "**Crucially, you must replace the placeholder API keys with your own for the search tools to function correctly.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "66130b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "import sys\n",
    "import inspect\n",
    "import threading\n",
    "from io import StringIO\n",
    "from typing import Any, Dict, List, Union, Optional, Tuple\n",
    "from abc import ABC, abstractmethod\n",
    "from contextlib import contextmanager\n",
    "\n",
    "# Pydantic and API Libraries\n",
    "from pydantic import BaseModel\n",
    "from openai import OpenAI\n",
    "from tenacity import retry, stop_after_attempt, wait_random_exponential\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import wikipedia\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "import numpy as np\n",
    "import json_repair # For fixing malformed JSON from the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c98b43df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notebook configured for model 'qwen-1.5-finetuned-grpo' at http://localhost:8000\n"
     ]
    }
   ],
   "source": [
    "# --- Core Configuration ---\n",
    "# The base URL where your vLLM server is running.\n",
    "VLLM_BASE_URL = \"http://localhost:8000\"\n",
    "# The API key for your vLLM server (can be a dummy key if not required by your setup).\n",
    "VLLM_API_KEY = \"a-secret-key\"\n",
    "# The exact name of the model being served by vLLM or your fine-tuned model. This should match the model name configured in your vLLM server.\n",
    "MODEL_NAME = \"qwen-1.5-finetuned-grpo\"\n",
    "\n",
    "# --- Environment Variables for Tools ---\n",
    "# IMPORTANT: You must provide your own API keys for the search tools to function.\n",
    "# If you leave these as placeholders, the corresponding tools will operate in a 'mock' mode.\n",
    "os.environ['OPENAI_API_KEY'] = 'YOUR_OPENAI_API_KEY_HERE' # Needed for embeddings in Web_Search_Tool\n",
    "os.environ['GOOGLE_API_KEY'] = 'YOUR_GOOGLE_API_KEY_HERE' # Needed for Google_Search_Tool\n",
    "\n",
    "print(f\"Notebook configured for model '{MODEL_NAME}' at {VLLM_BASE_URL}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "engine-1",
   "metadata": {},
   "source": [
    "## Section 2: The Core LLM Engine\n",
    "\n",
    "To interact with our LLM, we'll create a wrapper class. This is a crucial design pattern that provides several benefits:\n",
    "- **Abstraction:** It hides the specific details of the API calls, allowing us to easily swap out the backend (e.g., from vLLM to another provider) without changing the rest of our code.\n",
    "- **Robustness:** We can build in features like automatic retries for failed API calls.\n",
    "- **Feature Enhancement:** We can add custom logic, such as forcing the LLM to produce structured JSON output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "engine-abc",
   "metadata": {},
   "source": [
    "### 2.1 Abstract Base Class for the Engine\n",
    "\n",
    "We start with an Abstract Base Class (ABC) to define a standard interface that all our LLM engine wrappers must follow. This ensures consistency and interchangeability. Any class that inherits from `EngineLM` *must* implement the `generate` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1333dc5f-abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EngineLM(ABC):\n",
    "    \"\"\"An abstract base class for a language model engine.\"\"\"\n",
    "    \n",
    "    def __call__(self, *args, **kwargs):\n",
    "        \"\"\"Allows the class instance to be called like a function, making the syntax cleaner.\"\"\"\n",
    "        return self.generate(*args, **kwargs)\n",
    "\n",
    "    @abstractmethod\n",
    "    def generate(self, prompt, system_prompt=None, **kwargs):\n",
    "        \"\"\"The core method that must be implemented by any subclass. It takes a prompt and generates a response.\"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "engine-vllm",
   "metadata": {},
   "source": [
    "### 2.2 The vLLM Chat Engine Implementation\n",
    "\n",
    "This is our concrete implementation for interacting with the vLLM server. It handles prompt formatting, API calls, and, most importantly, the logic for parsing structured JSON output. This class is the heart of our agent's \"brain\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1333dc5f-impl",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChatVLLM(EngineLM):\n",
    "    \"\"\"A language model engine that connects to a vLLM server with an OpenAI-compatible API.\"\"\"\n",
    "    \n",
    "    def __init__(self, model_string, base_url, api_key, temperature=0.0):\n",
    "        \"\"\"Initializes the engine with connection details and the OpenAI client.\"\"\"\n",
    "        self.model_string = model_string\n",
    "        self.base_url = base_url\n",
    "        self.api_key = api_key\n",
    "        self.temperature = temperature\n",
    "        # The OpenAI client is configured to point to our local vLLM server.\n",
    "        self.client = OpenAI(base_url=self.base_url, api_key=self.api_key)\n",
    "        self.default_system_prompt = \"You are a helpful, creative, and smart assistant.\"\n",
    "\n",
    "    @retry(wait=wait_random_exponential(min=1, max=5), stop=stop_after_attempt(3))\n",
    "    def generate(self, content: Union[str, List[Union[str, bytes]]], system_prompt=None, response_format=None, **kwargs):\n",
    "        \"\"\"Generates a response from the LLM, with robust JSON parsing and retry logic.\"\"\"\n",
    "        # Use the provided system prompt or fall back to the default.\n",
    "        sys_prompt_arg = system_prompt if system_prompt else self.default_system_prompt\n",
    "        user_content = content\n",
    "\n",
    "        # Format the request in the standard chat completions message format.\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": sys_prompt_arg},\n",
    "            {\"role\": \"user\", \"content\": user_content}\n",
    "        ]\n",
    "\n",
    "        # Prepare the parameters for the API request.\n",
    "        request_params = {\n",
    "            \"model\": self.model_string,\n",
    "            \"messages\": messages,\n",
    "            \"temperature\": self.temperature,\n",
    "            \"max_tokens\": kwargs.get(\"max_tokens\", 4096),\n",
    "        }\n",
    "\n",
    "        # This block is key for structured output. If a Pydantic model is provided,\n",
    "        # we serialize its JSON schema and append it to the prompt, instructing the LLM\n",
    "        # to format its response accordingly. This is a form of \"prompt engineering\".\n",
    "        if response_format and issubclass(response_format, BaseModel):\n",
    "            json_schema = response_format.model_json_schema()\n",
    "            schema_instruction = (f\"\\n\\nPlease format your entire response as a single JSON object that strictly adheres to the following Pydantic schema.\\n\"+\n",
    "                                f\"Do not include any other text, explanations, or markdown formatting outside of the JSON object.\\n\\n\"+\n",
    "                                f\"Schema:\\n```json\\n{json.dumps(json_schema, indent=2)}\\n```\")\n",
    "            # Append the instruction to the last user message.\n",
    "            if isinstance(request_params['messages'][-1]['content'], str):\n",
    "                 request_params['messages'][-1]['content'] += schema_instruction\n",
    "\n",
    "        print(f\"\\n{'─'*50}\\n[LLM_INPUT] Sending request to model: {self.model_string}\\n{'─'*50}\")\n",
    "        \n",
    "        try:\n",
    "            # Execute the API call to the vLLM server.\n",
    "            response = self.client.chat.completions.create(**request_params)\n",
    "            response_text = response.choices[0].message.content\n",
    "        except Exception as e:\n",
    "            print(f\"[LLM ERROR] API Call Failed: {e}\")\n",
    "            raise e\n",
    "\n",
    "        print(f\"\\n{'─'*50}\\n[LLM_OUTPUT] Raw response from model:\\n{response_text}\\n{'─'*50}\")\n",
    "\n",
    "        # If we requested a structured format, we now parse the response.\n",
    "        if response_format and issubclass(response_format, BaseModel):\n",
    "            # First, try to extract the JSON from within markdown code blocks (e.g., ```json ... ```).\n",
    "            match = re.search(r\"```json\\s*(.*?)\\s*```\", response_text, re.DOTALL)\n",
    "            json_str = match.group(1) if match else response_text\n",
    "            \n",
    "            try:\n",
    "                # Attempt to parse the JSON strictly using the Pydantic model. This enforces the schema.\n",
    "                parsed_obj = response_format.model_validate_json(json_str.strip())\n",
    "                print(\"[LLM Engine] Successfully parsed structured output (Strict).\")\n",
    "                return parsed_obj\n",
    "            except Exception as e:\n",
    "                # If strict parsing fails (e.g., due to trailing commas, missing quotes), we attempt to repair it.\n",
    "                print(f\"[LLM Engine info] Strict parsing failed ({str(e)[:100]}...). Attempting repair...\")\n",
    "                try:\n",
    "                    # The `json_repair` library can fix many common LLM-generated JSON errors.\n",
    "                    decoded_obj = json_repair.loads(json_str)\n",
    "                    # After repairing, we validate the repaired object against the Pydantic model again.\n",
    "                    parsed_obj = response_format.model_validate(decoded_obj)\n",
    "                    print(\"[LLM Engine] Successfully parsed structured output (Repaired).\")\n",
    "                    return parsed_obj\n",
    "                except Exception as e2:\n",
    "                    # If even the repair fails, we log a critical warning. Returning the raw text allows\n",
    "                    # the agent to potentially see the error, but it might crash the next step.\n",
    "                    print(f\"[LLM Engine CRITICAL WARNING] Failed to parse output even with repair: {e2}\")\n",
    "                    return response_text\n",
    "\n",
    "        # If no structured format was requested, return the raw text response.\n",
    "        return response_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "engine-factory",
   "metadata": {},
   "source": [
    "### 2.3 Engine Factory and Connection Test\n",
    "\n",
    "A factory function is a clean way to instantiate our engine. We then immediately test the connection to the vLLM server to ensure everything is configured correctly before proceeding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1333dc5f-factory",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Testing vLLM engine connection ---\n",
      "\n",
      "──────────────────────────────────────────────────\n",
      "[LLM_INPUT] Sending request to model: qwen-1.5-finetuned-grpo\n",
      "──────────────────────────────────────────────────\n",
      "\n",
      "──────────────────────────────────────────────────\n",
      "[LLM_OUTPUT] Raw response from model:\n",
      "Pong\n",
      "──────────────────────────────────────────────────\n",
      "\n",
      "✅ Connection successful!\n"
     ]
    }
   ],
   "source": [
    "def create_llm_engine(model_string: str, **kwargs) -> ChatVLLM:\n",
    "    \"\"\"Factory function to create an instance of our vLLM chat engine.\"\"\"\n",
    "    return ChatVLLM(model_string=model_string, base_url=VLLM_BASE_URL, api_key=VLLM_API_KEY, temperature=kwargs.get('temperature', 0.0))\n",
    "\n",
    "# --- Test Connection ---\n",
    "try:\n",
    "    print(\"--- Testing vLLM engine connection ---\")\n",
    "    # Create an engine instance.\n",
    "    test_engine = create_llm_engine(MODEL_NAME)\n",
    "    # Send a simple message to see if we get a response.\n",
    "    test_response = test_engine.generate(\"Ping\")\n",
    "    print(f\"\\n✅ Connection successful!\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n❌ CONNECTION FAILED: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pydantic-1",
   "metadata": {},
   "source": [
    "## Section 3: Defining the Agent's Data Structures (Pydantic Models)\n",
    "\n",
    "A robust agentic system relies on predictable, structured data flowing between its components. We use Pydantic models to define the schemas for these data structures. This provides:\n",
    "\n",
    "- **Type Safety:** Ensures data conforms to the expected types (e.g., a field is a string, not a list).\n",
    "- **Validation:** Automatically checks if the data from the LLM is valid and complete.\n",
    "- **Self-Documentation:** The models themselves serve as clear documentation for what each component expects as input and produces as output.\n",
    "- **Reliable Communication:** They form the contract between the different LLM-powered \"roles\" in our system (Planner, Verifier, etc.)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pydantic-qanalysis",
   "metadata": {},
   "source": [
    "### 3.1 Query Analysis Model\n",
    "\n",
    "This model defines the structure of the initial analysis of the user's query. It's the output of the first step, where the Planner agent breaks down the problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "07a1728c-qanalysis",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QueryAnalysis(BaseModel):\n",
    "    \"\"\"Represents the initial breakdown and analysis of the user's query.\"\"\"\n",
    "    concise_summary: str\n",
    "    required_skills: str\n",
    "    relevant_tools: str\n",
    "    additional_considerations: str"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pydantic-nextstep",
   "metadata": {},
   "source": [
    "### 3.2 Next Step Model\n",
    "\n",
    "At each iteration of the main loop, the Planner agent decides on the next action. This model captures that decision, specifying which tool to use and for what purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "07a1728c-nextstep",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NextStep(BaseModel):\n",
    "    \"\"\"Defines the plan for the next action to be taken in the agent's loop.\"\"\"\n",
    "    justification: str\n",
    "    context: str\n",
    "    sub_goal: str\n",
    "    tool_name: str"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pydantic-toolcmd",
   "metadata": {},
   "source": [
    "### 3.3 Tool Command Model\n",
    "\n",
    "Once a tool and sub-goal are chosen, the Executor agent's job is to generate the exact code to run that tool. This model structures the output of the Executor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "07a1728c-toolcmd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToolCommand(BaseModel):\n",
    "    \"\"\"Represents the generated command for a specific tool, ready for execution.\"\"\"\n",
    "    analysis: str\n",
    "    explanation: str\n",
    "    command: str"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pydantic-memver",
   "metadata": {},
   "source": [
    "### 3.4 Memory Verification Model\n",
    "\n",
    "After an action is executed, the Verifier agent reflects on the current state. This model captures its conclusion: should the agent stop, or does it need to continue with more steps?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "07a1728c-memver",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MemoryVerification(BaseModel):\n",
    "    \"\"\"Represents the verifier's analysis of whether the task is complete.\"\"\"\n",
    "    analysis: str\n",
    "    stop_signal: bool"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pydantic-selectq",
   "metadata": {},
   "source": [
    "### 3.5 Relevant Queries Model (for Wikipedia Tool)\n",
    "\n",
    "This is a specialized model used internally by the `Wikipedia_Search_Tool`. When searching Wikipedia, it gets a list of possible page titles and uses an LLM call with this response format to select the most relevant pages to investigate further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "07a1728c-selectq",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Select_Relevant_Queries(BaseModel):\n",
    "    \"\"\"A specialized model for the Wikipedia tool to select relevant search results.\"\"\"\n",
    "    matched_queries: list[str]\n",
    "    matched_query_ids: list[int]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "memory-1",
   "metadata": {},
   "source": [
    "## Section 4: The Agent's Memory\n",
    "\n",
    "An agent needs a way to remember what it has done. The `Memory` class serves as a simple logbook, recording each action taken, the tool used, the command executed, and the result obtained. This history is fed back into the Planner and Verifier agents in subsequent steps, providing them with the context needed to make informed decisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "07a1728c-memory",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Memory:\n",
    "    \"\"\"A simple class to store the history of actions taken by the agent.\"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"Initializes an empty dictionary to store actions.\"\"\"\n",
    "        self.actions: Dict[str, Dict[str, Any]] = {}\n",
    "        \n",
    "    def add_action(self, step_count: int, tool_name: str, sub_goal: str, command: str, result: Any) -> None:\n",
    "        \"\"\"Adds a new action to the memory log.\"\"\"\n",
    "        self.actions[f\"Action Step {step_count}\"] = {\n",
    "            'tool_name': tool_name, \n",
    "            'sub_goal': sub_goal, \n",
    "            'command': command, \n",
    "            'result': result\n",
    "        }\n",
    "        \n",
    "    def get_actions(self) -> Dict[str, Dict[str, Any]]:\n",
    "        \"\"\"Retrieves the entire history of actions.\"\"\"\n",
    "        return self.actions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "memory-helper",
   "metadata": {},
   "source": [
    "### 4.1 Memory Serialization Helper\n",
    "\n",
    "Tool outputs can be complex objects or very long strings. Before storing a result in memory (and feeding it back into an LLM's limited context window), it's essential to serialize it into a clean, truncated JSON format. This function handles that conversion recursively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "07a1728c-serializer",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_json_serializable_truncated(obj, max_length: int = 2000):\n",
    "    \"\"\"Recursively converts an object into a JSON-serializable and truncated format.\"\"\"\n",
    "    # Handle basic, JSON-native types.\n",
    "    if isinstance(obj, (int, float, bool, type(None))): return obj\n",
    "    # Truncate long strings.\n",
    "    elif isinstance(obj, str): return obj if len(obj) <= max_length else obj[:max_length - 3] + \"...\"\n",
    "    # Recursively process dictionaries.\n",
    "    elif isinstance(obj, dict): return {str(k): make_json_serializable_truncated(v, max_length) for k, v in obj.items()}\n",
    "    # Recursively process lists.\n",
    "    elif isinstance(obj, list): return [make_json_serializable_truncated(element, max_length) for element in obj]\n",
    "    # For all other types, convert to a string representation and truncate.\n",
    "    else:\n",
    "        result = repr(obj)\n",
    "        return result if len(result) <= max_length else result[:max_length - 3] + \"...\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tools-1",
   "metadata": {},
   "source": [
    "## Section 5: Building the Tools\n",
    "\n",
    "Tools are what give an agent its power. They are functions that allow the agent to interact with the outside world, perform calculations, or access information beyond its own knowledge. By giving the agent access to tools, we ground its reasoning and enable it to solve much more complex problems.\n",
    "\n",
    "We start by defining a `BaseTool` abstract class to ensure all our tools have a consistent structure and expose metadata about their capabilities. This metadata is crucial for the Planner agent to decide which tool is appropriate for a given task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fb31198d-base",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseTool(ABC):\n",
    "    \"\"\"An abstract base class that defines the standard interface for all tools.\"\"\"\n",
    "    # A flag to indicate if the tool requires an LLM engine to function.\n",
    "    require_llm_engine = False\n",
    "    \n",
    "    def __init__(self, tool_name=None, tool_description=None, tool_version=None, input_types=None, output_type=None, demo_commands=None, user_metadata=None, model_string=None):\n",
    "        \"\"\"Initializes the tool with its descriptive metadata.\"\"\"\n",
    "        self.tool_name, self.tool_description, self.tool_version, self.input_types, self.output_type, self.demo_commands, self.user_metadata, self.model_string = tool_name, tool_description, tool_version, input_types, output_type, demo_commands, user_metadata, model_string\n",
    "    \n",
    "    def get_metadata(self) -> dict:\n",
    "        \"\"\"Returns all the tool's metadata in a dictionary. This is fed to the Planner agent.\"\"\"\n",
    "        return {\"tool_name\": self.tool_name, \"tool_description\": self.tool_description, \"tool_version\": self.tool_version, \"input_types\": self.input_types, \"output_type\": self.output_type, \"demo_commands\": self.demo_commands, \"user_metadata\": self.user_metadata, \"require_llm_engine\": self.require_llm_engine}\n",
    "    \n",
    "    def set_custom_output_dir(self, output_dir):\n",
    "        \"\"\"A placeholder method for tools that might need to save files.\"\"\"\n",
    "        pass\n",
    "        \n",
    "    @abstractmethod\n",
    "    def execute(self, *args, **kwargs):\n",
    "        \"\"\"The core method where the tool's logic is implemented. Must be overridden by subclasses.\"\"\"\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tools-generator",
   "metadata": {},
   "source": [
    "### 5.1 Generalist Solution Generator Tool\n",
    "\n",
    "This is the simplest tool. It's essentially a direct, unconstrained call to the LLM. It acts as a fallback or a general-purpose reasoning tool when no specialized tool is a good fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fb31198d-generator",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Base_Generator_Tool(BaseTool):\n",
    "    \"\"\"A general-purpose tool that uses an LLM to answer a query directly.\"\"\"\n",
    "    # This tool's primary function is to call an LLM, so it requires an engine.\n",
    "    require_llm_engine = True\n",
    "    \n",
    "    def __init__(self, model_string=\"gpt-4o-mini\"):\n",
    "        \"\"\"Initializes the tool's metadata and its own LLM engine.\"\"\"\n",
    "        super().__init__(\n",
    "            tool_name=\"Generalist_Solution_Generator_Tool\", \n",
    "            tool_description=\"A generalized tool that takes query from the user, and answers the question step by step to the best of its ability.\", \n",
    "            tool_version=\"1.0.0\", \n",
    "            input_types={\"query\": \"str\"}, \n",
    "            output_type=\"str\", \n",
    "            user_metadata={\n",
    "                \"limitation\": \"The Generalist_Solution_Generator_Tool may provide hallucinated or incorrect responses.\", \n",
    "                \"best_practice\": \"Use for general queries. Verify important information from its responses.\"\n",
    "            }\n",
    "        )\n",
    "        self.llm_engine = create_llm_engine(model_string, temperature=0.0)\n",
    "    \n",
    "    def execute(self, query, **kwargs):\n",
    "        \"\"\"Executes the tool by passing the query directly to its LLM engine.\"\"\"\n",
    "        return self.llm_engine.generate([query])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tools-python",
   "metadata": {},
   "source": [
    "### 5.2 Python Coder Tool\n",
    "\n",
    "This tool allows the agent to write and execute Python code. This is extremely powerful for tasks involving calculations, data manipulation, or logical operations. The implementation includes important safety features:\n",
    "\n",
    "- **Sandboxing:** The code is executed using `exec()` in a controlled, empty scope to prevent it from accessing or modifying the main program's state.\n",
    "- **Timeout:** A timer prevents the code from running indefinitely, which is crucial for handling infinite loops or long-running computations.\n",
    "- **Output Capturing:** It captures any `print` statements from the executed code, which is often how the result of a calculation is exposed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fb31198d-python",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeoutException(Exception):\n",
    "    \"\"\"Custom exception to be raised when an operation times out.\"\"\"\n",
    "    pass\n",
    "\n",
    "@contextmanager\n",
    "def timeout(seconds):\n",
    "    \"\"\"A context manager to enforce a timeout on a block of code.\"\"\"\n",
    "    # Define a function that will be called by the timer to raise the exception.\n",
    "    def raise_timeout(signum, frame):\n",
    "        raise TimeoutException(\"Code execution timed out\")\n",
    "    \n",
    "    # Use a threading.Timer to run the raise_timeout function after a delay.\n",
    "    timer = threading.Timer(seconds, lambda: raise_timeout(None, None))\n",
    "    timer.start()\n",
    "    try:\n",
    "        # The 'yield' passes control back to the 'with' block.\n",
    "        yield\n",
    "    finally:\n",
    "        # This code always runs, whether the 'with' block finished or an exception occurred.\n",
    "        # It's crucial to cancel the timer to prevent the timeout from firing later.\n",
    "        timer.cancel()\n",
    "\n",
    "class Python_Coder_Tool(BaseTool):\n",
    "    \"\"\"A tool to generate and execute Python code in a sandboxed environment.\"\"\"\n",
    "    require_llm_engine = True\n",
    "    \n",
    "    def __init__(self, model_string=\"gpt-4o\"):\n",
    "        super().__init__(\n",
    "            tool_name=\"Python_Code_Generator_Tool\", \n",
    "            tool_description=\"A tool that generates and executes simple Python code snippets for basic arithmetical calculations and math-related problems.\", \n",
    "            tool_version=\"1.0.0\", \n",
    "            input_types={\"query\": \"str\"}, \n",
    "            output_type=\"dict\", \n",
    "            user_metadata={\n",
    "                \"limitations\": \"Restricted to basic Python arithmetic and built-in math functions. Cannot use external libraries, file I/O, or network requests. Execution times out after 10 seconds.\", \n",
    "                \"best_practices\": \"Provide clear queries with all necessary numerical inputs. Good for math and logic problems.\"\n",
    "            }\n",
    "        )\n",
    "        self.llm_engine = create_llm_engine(model_string, temperature=0.0)\n",
    "\n",
    "    def execute(self, query, **kwargs):\n",
    "        # 1. Prompt an LLM to generate the Python code.\n",
    "        task_description = \"Given a query, generate a Python code snippet that performs the specified operation. Ensure to print the final result. The final output should be presented in the following format:\\n\\n```python\\n<code snippet>\\n```\"\n",
    "        full_prompt = f\"Task:\\n{task_description}\\n\\nQuery:\\n{query}\"\n",
    "        response = self.llm_engine.generate(full_prompt)\n",
    "        \n",
    "        # 2. Extract the code from the LLM's response.\n",
    "        match = re.search(r\"```python\\s*(.*?)\\s*```\", response, re.DOTALL)\n",
    "        if not match: return {\"error\": \"No Python code block found in the response\", \"raw_response\": response}\n",
    "        code_snippet = match.group(1).strip()\n",
    "        \n",
    "        # 3. Execute the code in a safe, controlled environment.\n",
    "        output_capture = StringIO() # Create an in-memory text buffer to capture print statements.\n",
    "        old_stdout, old_stderr = sys.stdout, sys.stderr # Store the original stdout/stderr\n",
    "        local_vars = {} # A dictionary to hold variables created by the executed code.\n",
    "        try:\n",
    "            # Redirect stdout and stderr to our in-memory buffer.\n",
    "            sys.stdout = sys.stderr = output_capture\n",
    "            with timeout(10): # Enforce a 10-second timeout.\n",
    "                # `exec` runs the code. We provide empty global and a local dict for the scope.\n",
    "                exec(code_snippet, {}, local_vars)\n",
    "            printed_output = output_capture.getvalue().strip()\n",
    "            # Return the captured output and any variables created by the code.\n",
    "            return {\"printed_output\": printed_output, \"variables\": {k: repr(v) for k, v in local_vars.items() if not k.startswith('__')}}\n",
    "        except TimeoutException as e: return {\"error\": str(e), \"code\": code_snippet}\n",
    "        except Exception as e: return {\"error\": str(e), \"code\": code_snippet, \"captured_output\": output_capture.getvalue().strip()}\n",
    "        finally: \n",
    "            # CRITICAL: Always restore the original stdout and stderr.\n",
    "            sys.stdout, sys.stderr = old_stdout, old_stderr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tools-google",
   "metadata": {},
   "source": [
    "### 5.3 Google Search Tool\n",
    "\n",
    "This tool gives the agent the ability to search the web for real-time information. It uses the Google Gemini API's built-in grounding feature, which is a simple and effective way to perform grounded generation based on search results. If no `GOOGLE_API_KEY` is provided, it will run in a mock mode, returning a placeholder string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b429b0-google",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Google_Search_Tool is in mock mode. Provide a GOOGLE_API_KEY to enable.\n"
     ]
    }
   ],
   "source": [
    "class Google_Search_Tool(BaseTool):\n",
    "    \"\"\"A tool for performing web searches using Google's Gemini API with grounding.\"\"\"\n",
    "    def __init__(self, model_string=\"gemini-1.5-flash\"):\n",
    "        super().__init__(\n",
    "            tool_name=\"Ground_Google_Search_Tool\", \n",
    "            tool_description=\"A web search tool powered by Google's Gemini AI that provides real-time information.\", \n",
    "            tool_version=\"1.0.0\", \n",
    "            input_types={\"query\": \"str\"}, \n",
    "            output_type=\"str\", \n",
    "            user_metadata={\"limitations\": \"Only suitable for general information search.\", \"best_practices\": \"Choose for question-type queries.\"}\n",
    "        )\n",
    "        self.search_model = model_string\n",
    "        \n",
    "        # Check for a valid API key. If not present, set client to None to enable mock mode.\n",
    "        if not os.getenv(\"GOOGLE_API_KEY\") or 'YOUR_GOOGLE_API_KEY' in os.getenv(\"GOOGLE_API_KEY\"):\n",
    "            print(\"WARNING: Google_Search_Tool is in mock mode. Provide a GOOGLE_API_KEY to enable.\")\n",
    "            self.client = None\n",
    "        else:\n",
    "            # We'll use the recommended `genai.GenerativeModel` for modern usage, but the logic is similar.\n",
    "            genai.configure(api_key=os.getenv(\"GOOGLE_API_KEY\"))\n",
    "            self.client = genai.GenerativeModel(self.search_model)\n",
    "\n",
    "    def execute(self, query: str, add_citations: bool = True, **kwargs):\n",
    "        # If in mock mode, return a placeholder response.\n",
    "        if not self.client: \n",
    "            return f\"[Mock Response] Search results for: '{query}'\"\n",
    "        \n",
    "        try:\n",
    "            # Configure the Gemini API to use its internal Google Search tool for grounding.\n",
    "            grounding_tool = genai.Tool(\n",
    "                google_search=genai.GoogleSearch()\n",
    "            )\n",
    "            # Generate content with the search tool enabled.\n",
    "            response = self.client.generate_content(\n",
    "                query,\n",
    "                tools=[grounding_tool],\n",
    "            )\n",
    "            return response.text\n",
    "        except Exception as e:\n",
    "            return f\"Error during Google Search: {e}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tools-web",
   "metadata": {},
   "source": [
    "### 5.4 Web RAG Search Tool\n",
    "\n",
    "This tool implements a complete Retrieval-Augmented Generation (RAG) pipeline to answer a query based on the content of a specific URL. It's more targeted than a general web search. The process is:\n",
    "\n",
    "1.  **Fetch & Parse:** Downloads the HTML from the URL and extracts all the clean text.\n",
    "2.  **Chunk:** Splits the long text into smaller, manageable chunks.\n",
    "3.  **Embed:** Uses an embedding model (from OpenAI in this case) to convert the user's query and each text chunk into numerical vectors.\n",
    "4.  **Retrieve:** Calculates the cosine similarity between the query vector and all chunk vectors to find the chunks that are most semantically relevant to the query.\n",
    "5.  **Synthesize:** Passes the original query and the content of the most relevant chunks to an LLM, asking it to synthesize a final answer based *only* on the provided context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "17b429b0-web",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Web_Search_Tool is in mock mode. Provide an OPENAI_API_KEY to enable embeddings.\n"
     ]
    }
   ],
   "source": [
    "class Web_Search_Tool(BaseTool):\n",
    "    \"\"\"Answers questions by retrieving info from a website using a RAG pipeline.\"\"\"\n",
    "    require_llm_engine = True\n",
    "    \n",
    "    def __init__(self, model_string=\"gpt-4o-mini\"):\n",
    "        super().__init__(tool_name=\"Web_RAG_Search_Tool\", tool_description=\"Answers questions by retrieving info from a website using RAG.\", tool_version=\"1.0.0\", input_types={\"query\": \"str\", \"url\": \"str\"}, output_type=\"str\", user_metadata={\"limitation\": \"May not work with JS-heavy sites or those requiring authentication.\", \"best_practice\": \"Use specific, targeted queries on text-rich websites.\"})\n",
    "        self.llm_engine = create_llm_engine(model_string, temperature=0.0)\n",
    "        # This tool requires an OpenAI key for its embedding model.\n",
    "        if not os.getenv(\"OPENAI_API_KEY\") or 'YOUR_OPENAI_API_KEY' in os.getenv(\"OPENAI_API_KEY\"):\n",
    "            print(\"WARNING: Web_Search_Tool is in mock mode. Provide an OPENAI_API_KEY to enable embeddings.\"); self.embedding_client = None\n",
    "        else: self.embedding_client = OpenAI()\n",
    "    \n",
    "    def execute(self, query, url, **kwargs):\n",
    "        if not self.embedding_client: return f\"[Mock Response] RAG summary for query '{query}' on URL '{url}'\"\n",
    "        try:\n",
    "            # 1. Fetch & Parse: Use requests and BeautifulSoup to get text from the URL.\n",
    "            headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}\n",
    "            content = BeautifulSoup(requests.get(url, headers=headers, timeout=10).content, 'html.parser').get_text(separator='\\n', strip=True)\n",
    "            \n",
    "            # 2. Chunk: Split the text into overlapping chunks of 200 words.\n",
    "            words = content.split(); chunks = [\" \".join(words[i:i+200]) for i in range(0, len(words), 180)]\n",
    "            \n",
    "            # 3. Embed: Get embeddings for the query and all chunks in a single API call.\n",
    "            embeddings = self.embedding_client.embeddings.create(input=[query] + chunks, model=\"text-embedding-3-small\").data\n",
    "            query_embedding, chunk_embeddings = np.array(embeddings[0].embedding), np.array([e.embedding for e in embeddings[1:]])\n",
    "            \n",
    "            # 4. Retrieve: Calculate cosine similarity and get the top 10 most relevant chunks.\n",
    "            similarities = [np.dot(query_embedding, ce) / (np.linalg.norm(query_embedding) * np.linalg.norm(ce)) for ce in chunk_embeddings]\n",
    "            top_chunks = [chunks[i] for i in np.argsort(similarities)[-10:][::-1]]\n",
    "            reference_info = \"\\n\".join([f\"[{i+1}] {chunk}\" for i, chunk in enumerate(top_chunks)])\n",
    "            \n",
    "            # 5. Synthesize: Prompt a separate LLM to generate an answer based on the retrieved chunks.\n",
    "            summary_prompt = f\"You are an expert AI assistant. Your task is to provide a clear, concise, and accurate answer to the user's query based **exclusively** on the provided reference information.\\n\\n## Step-by-Step Instructions\\n1.  **Analyze the Query:** First, fully understand the user's query and identify the specific information being asked for.\\n2.  **Scan for Relevance:** Read through each numbered chunk in the reference information. Identify all chunks that contain information directly relevant to answering the query.\\n3.  **Extract Key Facts & Synthesize:** From the relevant chunks, extract only the key facts and figures needed. Synthesize these extracted facts into a comprehensive, single-paragraph answer.\\n\\n---\\n## Your Turn\\n\\n### User Query\\n{query}\\n\\n### Reference Information\\n{reference_info}\\n\\n### Output\\n\"\n",
    "            return self.llm_engine.generate(summary_prompt)\n",
    "        except Exception as e: return f\"Error in Web_Search_Tool: {e}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tools-wiki",
   "metadata": {},
   "source": [
    "### 5.5 Wikipedia RAG Search Tool\n",
    "\n",
    "This is a *composite tool* or a *meta-tool*. It orchestrates other components to perform its task. Its process is:\n",
    "\n",
    "1.  **Search:** Use the `wikipedia` library to get a list of potential page titles related to the user's query.\n",
    "2.  **Select:** Use an LLM call (with the `Select_Relevant_Queries` Pydantic model) to intelligently filter this list down to the most promising candidates.\n",
    "3.  **Process:** For each selected page, it calls the `Web_Search_Tool` (which it holds an instance of) to perform a full RAG pipeline on that specific Wikipedia page.\n",
    "4.  **Aggregate:** It returns a structured dictionary containing the retrieved information from all the relevant pages it processed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "17b429b0-wiki",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Web_Search_Tool is in mock mode. Provide an OPENAI_API_KEY to enable embeddings.\n"
     ]
    }
   ],
   "source": [
    "class Wikipedia_Search_Tool(BaseTool):\n",
    "    \"\"\"A composite tool that searches Wikipedia, selects relevant pages, and applies RAG.\"\"\"\n",
    "    require_llm_engine = True\n",
    "    \n",
    "    def __init__(self, model_string=\"gpt-4o-mini\"):\n",
    "        super().__init__(tool_name=\"Wikipedia_RAG_Search_Tool\", tool_description=\"Searches Wikipedia and uses RAG to get grounded information from pages.\", tool_version=\"1.0.0\", input_types={\"query\": \"str\"}, output_type=\"dict\", user_metadata={\"limitation\": \"Wikipedia only. Accuracy depends on Wikipedia content. Filtering of pages depends on LLM performance.\", \"best_practice\": \"Use specific, targeted queries. Trust the 'relevant_pages' results.\"})\n",
    "        self.llm_engine = create_llm_engine(model_string, temperature=0.0)\n",
    "        # This tool internally uses another tool.\n",
    "        self.web_rag_tool = Web_Search_Tool(model_string=model_string)\n",
    "    \n",
    "    def execute(self, query, **kwargs):\n",
    "        try:\n",
    "            # 1. Search: Get up to 10 potential page titles from the Wikipedia API.\n",
    "            search_results = wikipedia.search(query, results=10)\n",
    "            if not search_results: return {\"error\": f\"No results found for '{query}'\"}\n",
    "            \n",
    "            # 2. Select: Prompt an LLM to choose the most relevant titles from the search results.\n",
    "            query_candidates_str = \"\\n\".join([f\"{i}. {query}\" for i, query in enumerate(search_results)])\n",
    "            prompt = f\"\"\"You are an expert AI assistant. Your task is to identify and select the most relevant queries from a list of Wikipedia search results that are most likely to address the user’s original question.\\n\\n## Input\\n\\nOriginal Query: `{query}`\\nQuery Candidates from Wikipedia Search: `{query_candidates_str}`\\n\\n## Instructions\\n1. Carefully read the original query and the list of query candidates.\\n2. Select the query candidates that are most relevant to the original query.\\n3. Return up to 3 most relevant queries.\"\"\"\n",
    "\n",
    "            selection = self.llm_engine.generate(prompt, response_format=Select_Relevant_Queries)\n",
    "            # Fallback logic in case the LLM fails to produce a valid structured response.\n",
    "            if not isinstance(selection, Select_Relevant_Queries):\n",
    "                print(\"Warning: Failed to parse relevant queries, using first result as fallback.\")\n",
    "                selection = Select_Relevant_Queries(matched_queries=[search_results[0]], matched_query_ids=[0])\n",
    "\n",
    "            # 3. Process & Aggregate: Loop through the selected titles.\n",
    "            relevant_pages = []\n",
    "            for title in selection.matched_queries:\n",
    "                try:\n",
    "                    # Get the full page object from the Wikipedia API.\n",
    "                    page = wikipedia.page(title, auto_suggest=False)\n",
    "                    # Use the Web_RAG_Tool to process the content of the page's URL.\n",
    "                    info = self.web_rag_tool.execute(query=query, url=page.url)\n",
    "                    relevant_pages.append({\"title\": title, \"url\": page.url, \"retrieved_information\": info})\n",
    "                except Exception as page_e: \n",
    "                    # Handle cases where a page might be a disambiguation page or cause an error.\n",
    "                    relevant_pages.append({\"title\": title, \"url\": f\"https://en.wikipedia.org/wiki/{title.replace(' ', '_')}\", \"error\": str(page_e)})\n",
    "            # Return a structured dictionary of the findings.\n",
    "            return {\"query\": query, \"relevant_pages\": relevant_pages}\n",
    "        except Exception as e: return {\"error\": f\"Wikipedia search failed: {e}\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "init-1",
   "metadata": {},
   "source": [
    "## Section 6: The Agent's Orchestration Engine\n",
    "\n",
    "Now we bring all the pieces together. The orchestration engine is responsible for managing the agent's lifecycle, from initializing the tools to running the main problem-solving loop."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "init-2",
   "metadata": {},
   "source": [
    "### 6.1 The Tool Initializer\n",
    "\n",
    "This class handles the setup of the agent's toolbox. It takes a list of desired tools, instantiates their respective classes, and stores both the tool instances (for execution) and their metadata (for the Planner agent). This separation of concerns keeps the main `Solver` class cleaner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9c63901c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Initializer:\n",
    "    \"\"\"Handles the loading and configuration of all tools for the agent.\"\"\"\n",
    "    def __init__(self, enabled_tools: List[str], tool_engine: List[str], model_string: str):\n",
    "        self.toolbox_metadata, self.tool_instances_cache, self.available_tools = {}, {}, []\n",
    "        print(\"\\n==> Initializing agent...\"); self._set_up_tools(enabled_tools, tool_engine, model_string)\n",
    "        \n",
    "    def _set_up_tools(self, enabled_tools: List[str], tool_engine: List[str], model_string: str):\n",
    "        print(f\"Enabled tools: {enabled_tools} with engines: {tool_engine}\")\n",
    "        # A registry of all available tool classes.\n",
    "        all_tool_classes = {\n",
    "            \"Base_Generator_Tool\": Base_Generator_Tool, \n",
    "            \"Python_Coder_Tool\": Python_Coder_Tool, \n",
    "            \"Google_Search_Tool\": Google_Search_Tool, \n",
    "            \"Wikipedia_RAG_Search_Tool\": Wikipedia_Search_Tool\n",
    "        }\n",
    "        # Loop through the list of tools to enable.\n",
    "        for i, tool_class_name in enumerate(enabled_tools):\n",
    "            if tool_class_name in all_tool_classes:\n",
    "                tool_class = all_tool_classes[tool_class_name]\n",
    "                # Determine which LLM engine this tool instance should use.\n",
    "                engine = tool_engine[i] if i < len(tool_engine) else model_string\n",
    "                print(f\"  -> Loading '{tool_class_name}' with engine '{engine}'...\")\n",
    "                # Create an instance of the tool class.\n",
    "                instance = tool_class() if engine == \"Default\" else tool_class(model_string=engine)\n",
    "                ext_name = instance.tool_name\n",
    "                # Store the instance for execution and its metadata for planning.\n",
    "                self.tool_instances_cache[ext_name] = instance\n",
    "                self.toolbox_metadata[ext_name] = instance.get_metadata()\n",
    "                self.available_tools.append(ext_name)\n",
    "                print(f\"     ✓ Loaded and cached as '{ext_name}'\")\n",
    "        print(f\"\\n✅ Tool setup complete. Final available tools: {self.available_tools}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "solver-1",
   "metadata": {},
   "source": [
    "### 6.2 The Solver: The Main Agent Loop\n",
    "\n",
    "The `Solver` class is the master orchestrator. It embodies the entire agentic workflow. It initializes all the necessary components and contains the primary `solve` method which executes the multi-step reasoning loop.\n",
    "\n",
    "The `__init__` method sets up the different \"roles\" of the agent by creating separate LLM engine instances for each task. While they all point to the same model in this notebook, in a more advanced system they could be different models specialized for planning, verification, or code generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "solver-init",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Solver:\n",
    "    \"\"\"The main class that orchestrates the entire agentic problem-solving workflow.\"\"\"\n",
    "    def __init__(self, planner_main_engine, planner_fixed_engine, verifier_engine, executor_engine, enabled_tools, tool_engine, max_steps=5):\n",
    "        \"\"\"Initializes all components of the agent: LLM engines, tools, and memory.\"\"\"\n",
    "        self.max_steps = max_steps\n",
    "        print(\"\\n==> Initializing LLM engines for different roles...\")\n",
    "        # Initialize an LLM engine for each distinct role in the workflow.\n",
    "        self.llm_planner_main = create_llm_engine(planner_main_engine); print(f\"  - Planner (Main):      {planner_main_engine}\")\n",
    "        self.llm_planner_fixed = create_llm_engine(planner_fixed_engine); print(f\"  - Planner (Fixed/Aux): {planner_fixed_engine}\")\n",
    "        self.llm_verifier = create_llm_engine(verifier_engine); print(f\"  - Verifier:            {verifier_engine}\")\n",
    "        self.llm_executor = create_llm_engine(executor_engine); print(f\"  - Executor:            {executor_engine}\")\n",
    "        \n",
    "        # Use the Initializer class to set up the toolbox.\n",
    "        initializer = Initializer(enabled_tools, tool_engine, planner_main_engine)\n",
    "        self.tool_instances_cache = initializer.tool_instances_cache\n",
    "        self.toolbox_metadata = initializer.toolbox_metadata\n",
    "        self.available_tools = initializer.available_tools\n",
    "        \n",
    "        # Initialize the agent's memory.\n",
    "        self.memory = Memory()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "solver-loop",
   "metadata": {},
   "source": [
    "### 6.3 The `solve` Method: Step-by-Step Execution\n",
    "\n",
    "This is the core logic. The `solve` method takes a user query and executes the agentic loop until the problem is solved or the maximum number of steps is reached. Let's break down each phase of the loop."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "solver-step0",
   "metadata": {},
   "source": [
    "#### Step 0: Initial Query Analysis\n",
    "Before the loop begins, the agent performs a one-time analysis of the query to create a high-level understanding of the user's intent, the skills required, and the tools that might be relevant. This analysis provides context for all subsequent steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "solver-step0-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve_step_0_analyze(self, question: str) -> QueryAnalysis:\n",
    "    \"\"\"Performs the initial analysis of the user's query.\"\"\"\n",
    "    print(f\"\\n{'='*80}\\n==> 🔍 Received Query: {question}\\n{'='*80}\")\n",
    "    # Clear memory from any previous runs.\n",
    "    self.memory.actions.clear()\n",
    "\n",
    "    print(\"\\n==> 🔍 Step 0: Query Analysis (using planner_fixed_engine)\")\n",
    "    # This is the prompt that instructs the LLM on how to analyze the query.\n",
    "    # It's given the query, the list of available tools, and the detailed tool metadata.\n",
    "    prompt_analyze_query = f\"\"\"Task: Analyze the given query to determine necessary skills and tools.\n",
    "\n",
    "Inputs:\n",
    "- Query: {question}\n",
    "- Available tools: {json.dumps(self.available_tools)}\n",
    "- Metadata for tools: {json.dumps(self.toolbox_metadata, indent=2)}\n",
    "\n",
    "Instructions:\n",
    "1. Identify the main objectives in the query.\n",
    "2. List the necessary skills and tools.\n",
    "3. For each skill and tool, explain how it helps address the query.\n",
    "4. Note any additional considerations.\n",
    "\n",
    "Format your response with a summary of the query, lists of skills and tools with explanations, and a section for additional considerations. Be brief and precise with insight.\"\"\"\n",
    "    # The LLM is asked to respond in the format of the QueryAnalysis Pydantic model.\n",
    "    query_analysis = self.llm_planner_fixed.generate(prompt_analyze_query, response_format=QueryAnalysis)\n",
    "    print(f\"\\n--- Analysis Result ---\\n{json.dumps(query_analysis.model_dump(), indent=2)}\")\n",
    "    return query_analysis\n",
    "\n",
    "# We will attach this method to the Solver class later.\n",
    "Solver.solve_step_0_analyze = solve_step_0_analyze"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "solver-step1",
   "metadata": {},
   "source": [
    "#### Step {i}.1: Planning the Next Action\n",
    "At the start of each loop iteration, the Planner agent decides what to do next. It considers the original query, the initial analysis, the history of all previous actions stored in memory, and the available tools. Its goal is to choose the single best tool and define a clear, specific sub-goal for it to achieve in this step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "solver-step1-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve_step_1_plan(self, question: str, query_analysis: QueryAnalysis, step_count: int) -> NextStep:\n",
    "    \"\"\"Plans the next best action for the current step of the loop.\"\"\"\n",
    "    print(f\"\\n{'='*80}\\n==> 🎯 Step {step_count}: Planning Next Action (using planner_main_engine)\\n{'='*80}\")\n",
    "            \n",
    "    # The prompt for the Planner. It receives all current context.\n",
    "    prompt_next_step = f\"\"\"Task: Determine the optimal next step to address the query using available tools and previous steps.\n",
    "\n",
    "Context:\n",
    "- **Query:** {question}\n",
    "- **Query Analysis:** {query_analysis}\n",
    "- **Available Tools:** {json.dumps(self.available_tools)}\n",
    "- **Toolbox Metadata:** {json.dumps(self.toolbox_metadata, indent=2)}\n",
    "- **Previous Steps:** {json.dumps(self.memory.get_actions())}\n",
    "\n",
    "Instructions:\n",
    "1. Analyze the query, previous steps, and available tools.\n",
    "2. Select the **single best tool** for the next step.\n",
    "3. Formulate a specific, achievable **sub-goal** for that tool.\n",
    "4. Provide all necessary **context** (data, file names, variables) for the tool to function.\n",
    "\n",
    "Response Format:\n",
    "1.  **Justification:** Explain your choice of tool and sub-goal.\n",
    "2.  **Context:** Provide all necessary information for the tool.\n",
    "3.  **Sub-Goal:** State the specific objective for the tool.\n",
    "4.  **Tool Name:** State the exact name of the selected tool.\"\"\"\n",
    "    # The LLM must respond in the format of the NextStep Pydantic model.\n",
    "    next_step = self.llm_planner_main.generate(prompt_next_step, response_format=NextStep)\n",
    "    print(f\"[Planner Justification]: {next_step.justification}\\n[Selected Tool]: {next_step.tool_name}\\n[Sub-Goal]: {next_step.sub_goal}\")\n",
    "    return next_step\n",
    "\n",
    "Solver.solve_step_1_plan = solve_step_1_plan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "solver-step2",
   "metadata": {},
   "source": [
    "#### Step {i}.2: Generating the Tool Command\n",
    "Once a tool and sub-goal are chosen, the Executor agent takes over. Its job is to translate the high-level sub-goal into a precise, executable piece of Python code that calls the selected tool with the correct arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "solver-step2-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve_step_2_generate_command(self, question: str, next_step: NextStep, step_count: int) -> str:\n",
    "    \"\"\"Generates the executable Python command for the chosen tool.\"\"\"\n",
    "    print(f\"\\n==> 📝 Step {step_count}: Generating Command for '{next_step.tool_name}' (using executor_engine)\")\n",
    "    # The prompt for the Executor. It gets the sub-goal, tool metadata, and relevant context.\n",
    "    prompt_tool_command = f\"\"\"Task: Generate a precise command to execute the selected tool.\n",
    "\n",
    "Context:\n",
    "- **Query:** {question}\n",
    "- **Sub-Goal:** {next_step.sub_goal}\n",
    "- **Tool Name:** {next_step.tool_name}\n",
    "- **Tool Metadata:** {self.toolbox_metadata.get(next_step.tool_name, {})}\n",
    "- **Relevant Data:** {next_step.context}\n",
    "\n",
    "Instructions:\n",
    "1.  Analyze the tool's required parameters from its metadata.\n",
    "2.  Construct valid Python code that addresses the sub-goal using the provided context and data.\n",
    "3.  The command must include at least one call to `tool.execute()`.\n",
    "4.  Each `tool.execute()` call must be assigned to a variable named **`execution`**.\n",
    "5.  Please give the exact numbers and parameters should be used in the `tool.execute()` call.\n",
    "\"\"\"\n",
    "    # The LLM must respond in the format of the ToolCommand Pydantic model.\n",
    "    tool_command_obj = self.llm_executor.generate(prompt_tool_command, response_format=ToolCommand)\n",
    "    command_to_run = tool_command_obj.command.strip()\n",
    "    print(f\"[Generated Command]:\\n```python\\n{command_to_run}\\n```\")\n",
    "    return command_to_run\n",
    "\n",
    "Solver.solve_step_2_generate_command = solve_step_2_generate_command"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "solver-step3",
   "metadata": {},
   "source": [
    "#### Step {i}.3: Executing the Command\n",
    "This is where the agent interacts with the world. The generated command is executed. The tool instance is made available in the local scope of the `exec` call, and the result is captured. Any errors during execution are caught and stored in the result, so the agent can see that its command failed and potentially correct it in the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "solver-step3-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve_step_3_execute_command(self, next_step: NextStep, command_to_run: str, step_count: int):\n",
    "    \"\"\"Executes the generated command and stores the result in memory.\"\"\"\n",
    "    print(f\"\\n==> 🛠️ Step {step_count}: Executing Command for '{next_step.tool_name}'\")\n",
    "    tool_instance = self.tool_instances_cache.get(next_step.tool_name)\n",
    "    local_context = {'tool': tool_instance}\n",
    "    \n",
    "    if not tool_instance:\n",
    "        result = f\"Error: Tool '{next_step.tool_name}' not found.\"\n",
    "    else:\n",
    "        try:\n",
    "            # Execute the command. The result must be stored in a variable named 'execution'.\n",
    "            exec(command_to_run, {}, local_context)\n",
    "            result = local_context.get('execution', \"Error: No 'execution' variable returned.\")\n",
    "        except Exception as e:\n",
    "            result = f\"Execution Error: {str(e)}\"\n",
    "    \n",
    "    # Sanitize and truncate the result before adding it to memory.\n",
    "    serializable_result = make_json_serializable_truncated(result)\n",
    "    self.memory.add_action(step_count, next_step.tool_name, next_step.sub_goal, command_to_run, serializable_result)\n",
    "    print(f\"[Execution Result]:\\n{json.dumps(serializable_result, indent=2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "solver-step4",
   "metadata": {},
   "source": [
    "#### Step {i}.4: Verifying and Reflecting\n",
    "After executing an action, the agent must pause and reflect. The Verifier agent reviews everything—the original query, the initial analysis, and the full memory of actions and results—to determine if the query has been fully answered. Its output is a simple but critical boolean: `stop_signal`. If `True`, the loop terminates. If `False`, the agent proceeds to the next iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "solver-step4-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve_step_4_verify(self, question: str, query_analysis: QueryAnalysis, step_count: int) -> bool:\n",
    "    \"\"\"Verifies if the task is complete, returning True to stop or False to continue.\"\"\"\n",
    "    print(f\"\\n==> 🤖 Step {step_count}: Verifying Context (using verifier_engine)\")\n",
    "    # The prompt for the Verifier. It sees the full state of the problem.\n",
    "    prompt_verify = f\"\"\"Task: Evaluate if the current memory is complete and accurate enough to answer the query, or if more tools are needed.\n",
    "\n",
    "Context:\n",
    "- **Query:** {question}\n",
    "- **Available Tools:** {json.dumps(self.available_tools)}\n",
    "- **Toolbox Metadata:** {json.dumps(self.toolbox_metadata, indent=2)}\n",
    "- **Initial Analysis:** {query_analysis}\n",
    "- **Memory (Tools Used & Results):** {json.dumps(self.memory.get_actions())}\n",
    "\n",
    "Instructions:\n",
    "1.  Review the query, initial analysis, and memory.\n",
    "2.  Assess the completeness of the memory: Does it fully address all parts of the query?\n",
    "3.  Determine if any unused tools could provide missing information.\n",
    "4.  If the memory is sufficient, explain why and set 'stop_signal' to true.\n",
    "5.  If more information is needed, explain what's missing, which tools could help, and set 'stop_signal' to false.\n",
    "\"\"\"\n",
    "    # The LLM must respond in the format of the MemoryVerification Pydantic model.\n",
    "    verification = self.llm_verifier.generate(prompt_verify, response_format=MemoryVerification)\n",
    "    conclusion = 'STOP' if verification.stop_signal else 'CONTINUE'\n",
    "    print(f\"[Verifier Analysis]: {verification.analysis}\\n[Verifier Conclusion]: {conclusion}\")\n",
    "    return verification.stop_signal\n",
    "\n",
    "Solver.solve_step_4_verify = solve_step_4_verify"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "solver-final",
   "metadata": {},
   "source": [
    "#### Final Step: Synthesizing the Answer\n",
    "Once the Verifier signals to stop, the loop terminates. The agent now has all the necessary information in its memory. The final step is to use one last LLM call to synthesize all the actions and results into a single, coherent, human-readable answer that directly addresses the original user query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "solver-final-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve_final_step_synthesize(self, question: str, query_analysis: QueryAnalysis) -> str:\n",
    "    \"\"\"Generates the final, synthesized answer for the user.\"\"\"\n",
    "    print(f\"\\n{'='*80}\\n==> 🐙 Generating Final Output (using planner_fixed_engine)\\n{'='*80}\")\n",
    "    # The prompt for the final synthesis. It gets the query and the complete action history.\n",
    "    prompt_final_output = f\"\"\"Task: Generate a concise final answer to the query based on all provided context.\n",
    "\n",
    "Context:\n",
    "- **Query:** {question}\n",
    "- **Initial Analysis:** {query_analysis}\n",
    "- **Actions Taken:** {json.dumps(self.memory.get_actions())}\n",
    "\n",
    "Instructions:\n",
    "1. Review the query and the results from all actions.\n",
    "2. Synthesize the key findings into a clear, step-by-step summary of the process.\n",
    "3. Provide a direct, precise answer to the original query.\n",
    "\n",
    "Output Structure:\n",
    "1.  **Process Summary:** A clear, step-by-step breakdown of how the query was addressed.\n",
    "2.  **Answer:** A direct and concise final answer to the query.\"\"\"\n",
    "    # This is a free-form generation, no Pydantic model is needed.\n",
    "    return self.llm_planner_fixed.generate(prompt_final_output)\n",
    "\n",
    "Solver.solve_final_step_synthesize = solve_final_step_synthesize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "solver-main-method",
   "metadata": {},
   "source": [
    "#### The Main `solve` Method\n",
    "\n",
    "Finally, we assemble all the previously defined step methods into the main `solve` method. This method orchestrates the calls to each step function in the correct order, manages the loop, and returns the final answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "solver-main-method-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve(self, question: str):\n",
    "    \"\"\"Main method to run the entire agentic loop from start to finish.\"\"\"\n",
    "    # Step 0: Initial Analysis\n",
    "    query_analysis = self.solve_step_0_analyze(question)\n",
    "    \n",
    "    step_count = 0\n",
    "    while step_count < self.max_steps:\n",
    "        step_count += 1\n",
    "        \n",
    "        # Step 1: Plan next action\n",
    "        next_step = self.solve_step_1_plan(question, query_analysis, step_count)\n",
    "        \n",
    "        # Step 2: Generate command\n",
    "        command_to_run = self.solve_step_2_generate_command(question, next_step, step_count)\n",
    "        \n",
    "        # Step 3: Execute command\n",
    "        self.solve_step_3_execute_command(next_step, command_to_run, step_count)\n",
    "        \n",
    "        # Step 4: Verify and decide whether to continue\n",
    "        should_stop = self.solve_step_4_verify(question, query_analysis, step_count)\n",
    "        if should_stop:\n",
    "            break\n",
    "            \n",
    "    # Final Step: Synthesize the final answer\n",
    "    final_answer = self.solve_final_step_synthesize(question, query_analysis)\n",
    "    return final_answer\n",
    "\n",
    "# Monkey-patch the main solve method onto the Solver class.\n",
    "Solver.solve = solve"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "run-1",
   "metadata": {},
   "source": [
    "## Section 7: Running the Agent\n",
    "\n",
    "With all the components built and assembled, it's time to run our agent. The `construct_solver` function brings together the configuration for our specific run—defining which LLM model to use for each role and which tools should be enabled. Then, we provide a query and let the agent work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b21a0dac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==> Initializing LLM engines for different roles...\n",
      "  - Planner (Main):      qwen-1.5-finetuned-grpo-fast\n",
      "  - Planner (Fixed/Aux): qwen-1.5-finetuned-grpo-fast\n",
      "  - Verifier:            qwen-1.5-finetuned-grpo-fast\n",
      "  - Executor:            qwen-1.5-finetuned-grpo-fast\n",
      "\n",
      "==> Initializing agent...\n",
      "Enabled tools: ['Base_Generator_Tool', 'Python_Coder_Tool', 'Google_Search_Tool', 'Wikipedia_RAG_Search_Tool'] with engines: ['qwen-1.5-finetuned-grpo-fast', 'qwen-1.5-finetuned-grpo-fast', 'Default', 'qwen-1.5-finetuned-grpo-fast']\n",
      "  -> Loading 'Base_Generator_Tool' with engine 'qwen-1.5-finetuned-grpo-fast'...\n",
      "     ✓ Loaded and cached as 'Generalist_Solution_Generator_Tool'\n",
      "  -> Loading 'Python_Coder_Tool' with engine 'qwen-1.5-finetuned-grpo-fast'...\n",
      "     ✓ Loaded and cached as 'Python_Code_Generator_Tool'\n",
      "  -> Loading 'Google_Search_Tool' with engine 'Default'...\n",
      "     ✓ Loaded and cached as 'Ground_Google_Search_Tool'\n",
      "  -> Loading 'Wikipedia_RAG_Search_Tool' with engine 'qwen-1.5-finetuned-grpo-fast'...\n",
      "WARNING: Web_Search_Tool is in mock mode. Provide an OPENAI_API_KEY to enable embeddings.\n",
      "     ✓ Loaded and cached as 'Wikipedia_RAG_Search_Tool'\n",
      "\n",
      "✅ Tool setup complete. Final available tools: ['Generalist_Solution_Generator_Tool', 'Python_Code_Generator_Tool', 'Ground_Google_Search_Tool', 'Wikipedia_RAG_Search_Tool']\n",
      "\n",
      "================================================================================\n",
      "==> 🔍 Received Query: Calculate 12 squared, and then use Wikipedia to find out what major historical event happened in that year (AD).\n",
      "================================================================================\n",
      "\n",
      "==> 🔍 Step 0: Query Analysis (using planner_fixed_engine)\n",
      "\n",
      "──────────────────────────────────────────────────\n",
      "[LLM_INPUT] Sending request to model: qwen-1.5-finetuned-grpo-fast\n",
      "──────────────────────────────────────────────────\n",
      "\n",
      "──────────────────────────────────────────────────\n",
      "[LLM_OUTPUT] Raw response from model:\n",
      "{\n",
      "  \"concise_summary\": \"The query requires calculating 12 squared and then finding a major historical event that occurred in the resulting year AD using Wikipedia.\",\n",
      "  \"required_skills\": \"Basic arithmetic calculation, ability to use Wikipedia for historical event research\",\n",
      "  \"relevant_tools\": \"Python_Code_Generator_Tool for calculating 12 squared, Wikipedia_RAG_Search_Tool for finding historical events on Wikipedia\",\n",
      "  \"additional_considerations\": \"Ensure the Python_Code_Generator_Tool can handle basic arithmetic operations like squaring a number, and verify that the Wikipedia_RAG_Search_Tool can accurately retrieve historical information for the specified year.\"\n",
      "}\n",
      "──────────────────────────────────────────────────\n",
      "[LLM Engine] Successfully parsed structured output (Strict).\n",
      "\n",
      "--- Analysis Result ---\n",
      "concise_summary='The query requires calculating 12 squared and then finding a major historical event that occurred in the resulting year AD using Wikipedia.' required_skills='Basic arithmetic calculation, ability to use Wikipedia for historical event research' relevant_tools='Python_Code_Generator_Tool for calculating 12 squared, Wikipedia_RAG_Search_Tool for finding historical events on Wikipedia' additional_considerations='Ensure the Python_Code_Generator_Tool can handle basic arithmetic operations like squaring a number, and verify that the Wikipedia_RAG_Search_Tool can accurately retrieve historical information for the specified year.'\n",
      "\n",
      "================================================================================\n",
      "==> 🎯 Step 1: Planning Next Action (using planner_main_engine)\n",
      "================================================================================\n",
      "\n",
      "──────────────────────────────────────────────────\n",
      "[LLM_INPUT] Sending request to model: qwen-1.5-finetuned-grpo-fast\n",
      "──────────────────────────────────────────────────\n",
      "\n",
      "──────────────────────────────────────────────────\n",
      "[LLM_OUTPUT] Raw response from model:\n",
      "```json\n",
      "{\n",
      "  \"justification\": \"The query requires calculating 12 squared, which is a basic arithmetic operation. The Python_Code_Generator_Tool is the most suitable tool for this task as it can generate and execute simple Python code snippets for basic arithmetical calculations. After calculating 12 squared, the result will be used to find a major historical event that occurred in the resulting year AD using Wikipedia, but the first step is to calculate 12 squared.\",\n",
      "  \"context\": \"Calculate 12 squared\",\n",
      "  \"sub_goal\": \"Calculate the square of 12\",\n",
      "  \"tool_name\": \"Python_Code_Generator_Tool\"\n",
      "}\n",
      "```\n",
      "──────────────────────────────────────────────────\n",
      "[LLM Engine] Successfully parsed structured output (Strict).\n",
      "[Planner Justification]: The query requires calculating 12 squared, which is a basic arithmetic operation. The Python_Code_Generator_Tool is the most suitable tool for this task as it can generate and execute simple Python code snippets for basic arithmetical calculations. After calculating 12 squared, the result will be used to find a major historical event that occurred in the resulting year AD using Wikipedia, but the first step is to calculate 12 squared.\n",
      "[Selected Tool]: Python_Code_Generator_Tool\n",
      "[Sub-Goal]: Calculate the square of 12\n",
      "\n",
      "==> 📝 Step 1: Generating Command for 'Python_Code_Generator_Tool' (using executor_engine)\n",
      "\n",
      "──────────────────────────────────────────────────\n",
      "[LLM_INPUT] Sending request to model: qwen-1.5-finetuned-grpo-fast\n",
      "──────────────────────────────────────────────────\n",
      "\n",
      "──────────────────────────────────────────────────\n",
      "[LLM_OUTPUT] Raw response from model:\n",
      "```json\n",
      "{\n",
      "  \"analysis\": \"The Python_Code_Generator_Tool requires a string input for the query. To calculate 12 squared, we need to construct a valid Python code snippet that performs this operation.\",\n",
      "  \"explanation\": \"We will use the tool's execute function to run the Python code. The code will be 'result = 12 ** 2' to calculate the square of 12. Then, we assign the execution of this code to the 'execution' variable.\",\n",
      "  \"command\": \"execution = tool.execute(query=\\\"result = 12 ** 2\\\")\"\n",
      "}\n",
      "```\n",
      "──────────────────────────────────────────────────\n",
      "[LLM Engine] Successfully parsed structured output (Strict).\n",
      "[Generated Command]:\n",
      "```python\n",
      "execution = tool.execute(query=\"result = 12 ** 2\")\n",
      "```\n",
      "\n",
      "==> 🛠️ Step 1: Executing Command for 'Python_Code_Generator_Tool'\n",
      "\n",
      "──────────────────────────────────────────────────\n",
      "[LLM_INPUT] Sending request to model: qwen-1.5-finetuned-grpo-fast\n",
      "──────────────────────────────────────────────────\n",
      "\n",
      "──────────────────────────────────────────────────\n",
      "[LLM_OUTPUT] Raw response from model:\n",
      "```python\n",
      "# Calculate the result of 12 squared\n",
      "result = 12 ** 2\n",
      "\n",
      "# Print the final result\n",
      "print(\"The result of 12 squared is:\", result)\n",
      "```\n",
      "──────────────────────────────────────────────────\n",
      "[Execution Result]:\n",
      "{\n",
      "  \"printed_output\": \"The result of 12 squared is: 144\",\n",
      "  \"variables\": {\n",
      "    \"result\": \"144\"\n",
      "  }\n",
      "}\n",
      "\n",
      "==> 🤖 Step 1: Verifying Context (using verifier_engine)\n",
      "\n",
      "──────────────────────────────────────────────────\n",
      "[LLM_INPUT] Sending request to model: qwen-1.5-finetuned-grpo-fast\n",
      "──────────────────────────────────────────────────\n",
      "\n",
      "──────────────────────────────────────────────────\n",
      "[LLM_OUTPUT] Raw response from model:\n",
      "{\"analysis\": \"The query requires two main steps: calculating 12 squared and finding a major historical event in the resulting year AD using Wikipedia. The memory contains the result of the first step, which is calculating 12 squared using the Python_Code_Generator_Tool, resulting in 144. However, it does not contain the result of the second step, which is finding a major historical event in the year 144 AD using Wikipedia. The Wikipedia_RAG_Search_Tool is suitable for this task but has not been used yet. Therefore, the memory is not complete, and more information is needed to fully address the query.\", \"stop_signal\": false}\n",
      "──────────────────────────────────────────────────\n",
      "[LLM Engine] Successfully parsed structured output (Strict).\n",
      "[Verifier Analysis]: The query requires two main steps: calculating 12 squared and finding a major historical event in the resulting year AD using Wikipedia. The memory contains the result of the first step, which is calculating 12 squared using the Python_Code_Generator_Tool, resulting in 144. However, it does not contain the result of the second step, which is finding a major historical event in the year 144 AD using Wikipedia. The Wikipedia_RAG_Search_Tool is suitable for this task but has not been used yet. Therefore, the memory is not complete, and more information is needed to fully address the query.\n",
      "[Verifier Conclusion]: CONTINUE\n",
      "\n",
      "================================================================================\n",
      "==> 🎯 Step 2: Planning Next Action (using planner_main_engine)\n",
      "================================================================================\n",
      "\n",
      "──────────────────────────────────────────────────\n",
      "[LLM_INPUT] Sending request to model: qwen-1.5-finetuned-grpo-fast\n",
      "──────────────────────────────────────────────────\n",
      "\n",
      "──────────────────────────────────────────────────\n",
      "[LLM_OUTPUT] Raw response from model:\n",
      "```json\n",
      "{\n",
      "  \"justification\": \"The Python_Code_Generator_Tool has already been used to calculate 12 squared, which resulted in 144. Now, we need to find a major historical event that occurred in the year AD 144. The Wikipedia_RAG_Search_Tool is the most suitable tool for this task as it can search Wikipedia and provide grounded information from pages. Therefore, the next step is to use the Wikipedia_RAG_Search_Tool to find historical events in the year 144.\",\n",
      "  \"context\": \"Year: 144, Previous calculation result: 144\",\n",
      "  \"sub_goal\": \"Find a major historical event that occurred in the year AD 144 using Wikipedia\",\n",
      "  \"tool_name\": \"Wikipedia_RAG_Search_Tool\"\n",
      "}\n",
      "```\n",
      "──────────────────────────────────────────────────\n",
      "[LLM Engine] Successfully parsed structured output (Strict).\n",
      "[Planner Justification]: The Python_Code_Generator_Tool has already been used to calculate 12 squared, which resulted in 144. Now, we need to find a major historical event that occurred in the year AD 144. The Wikipedia_RAG_Search_Tool is the most suitable tool for this task as it can search Wikipedia and provide grounded information from pages. Therefore, the next step is to use the Wikipedia_RAG_Search_Tool to find historical events in the year 144.\n",
      "[Selected Tool]: Wikipedia_RAG_Search_Tool\n",
      "[Sub-Goal]: Find a major historical event that occurred in the year AD 144 using Wikipedia\n",
      "\n",
      "==> 📝 Step 2: Generating Command for 'Wikipedia_RAG_Search_Tool' (using executor_engine)\n",
      "\n",
      "──────────────────────────────────────────────────\n",
      "[LLM_INPUT] Sending request to model: qwen-1.5-finetuned-grpo-fast\n",
      "──────────────────────────────────────────────────\n",
      "\n",
      "──────────────────────────────────────────────────\n",
      "[LLM_OUTPUT] Raw response from model:\n",
      "```json\n",
      "{\n",
      "  \"analysis\": \"The Wikipedia_RAG_Search_Tool requires a query as input and returns a dictionary as output. The tool's metadata suggests that it uses Wikipedia and RAG to get grounded information from pages. The sub-goal is to find a major historical event that occurred in the year AD 144.\",\n",
      "  \"explanation\": \"To achieve the sub-goal, we need to construct a query that targets the year 144 and execute the Wikipedia_RAG_Search_Tool with this query. The previous calculation result of 144 will be used as the year in the query.\",\n",
      "  \"command\": \"execution = tool.execute(query='major historical events in 144 AD')\"\n",
      "}\n",
      "```\n",
      "──────────────────────────────────────────────────\n",
      "[LLM Engine] Successfully parsed structured output (Strict).\n",
      "[Generated Command]:\n",
      "```python\n",
      "execution = tool.execute(query='major historical events in 144 AD')\n",
      "```\n",
      "\n",
      "==> 🛠️ Step 2: Executing Command for 'Wikipedia_RAG_Search_Tool'\n",
      "\n",
      "──────────────────────────────────────────────────\n",
      "[LLM_INPUT] Sending request to model: qwen-1.5-finetuned-grpo-fast\n",
      "──────────────────────────────────────────────────\n",
      "\n",
      "──────────────────────────────────────────────────\n",
      "[LLM_OUTPUT] Raw response from model:\n",
      "{\"matched_queries\": [], \"matched_query_ids\": []}\n",
      "──────────────────────────────────────────────────\n",
      "[LLM Engine] Successfully parsed structured output (Strict).\n",
      "[Execution Result]:\n",
      "{\n",
      "  \"query\": \"major historical events in 144 AD\",\n",
      "  \"relevant_pages\": []\n",
      "}\n",
      "\n",
      "==> 🤖 Step 2: Verifying Context (using verifier_engine)\n",
      "\n",
      "──────────────────────────────────────────────────\n",
      "[LLM_INPUT] Sending request to model: qwen-1.5-finetuned-grpo-fast\n",
      "──────────────────────────────────────────────────\n",
      "\n",
      "──────────────────────────────────────────────────\n",
      "[LLM_OUTPUT] Raw response from model:\n",
      "{\"analysis\": \"The query requires calculating 12 squared and then finding a major historical event that occurred in the resulting year AD using Wikipedia. The memory contains the result of 12 squared, which is 144, calculated using the Python_Code_Generator_Tool. However, the Wikipedia_RAG_Search_Tool was unable to find any relevant pages for major historical events in 144 AD. Given that the Wikipedia_RAG_Search_Tool did not provide the required historical information, the memory does not fully address all parts of the query. The Ground_Google_Search_Tool could potentially be used to find the historical event, as it provides real-time information and can be used for general information searches. Therefore, more information is needed to fully answer the query.\", \"stop_signal\": false}\n",
      "──────────────────────────────────────────────────\n",
      "[LLM Engine] Successfully parsed structured output (Strict).\n",
      "[Verifier Analysis]: The query requires calculating 12 squared and then finding a major historical event that occurred in the resulting year AD using Wikipedia. The memory contains the result of 12 squared, which is 144, calculated using the Python_Code_Generator_Tool. However, the Wikipedia_RAG_Search_Tool was unable to find any relevant pages for major historical events in 144 AD. Given that the Wikipedia_RAG_Search_Tool did not provide the required historical information, the memory does not fully address all parts of the query. The Ground_Google_Search_Tool could potentially be used to find the historical event, as it provides real-time information and can be used for general information searches. Therefore, more information is needed to fully answer the query.\n",
      "[Verifier Conclusion]: CONTINUE\n",
      "\n",
      "================================================================================\n",
      "==> 🎯 Step 3: Planning Next Action (using planner_main_engine)\n",
      "================================================================================\n",
      "\n",
      "──────────────────────────────────────────────────\n",
      "[LLM_INPUT] Sending request to model: qwen-1.5-finetuned-grpo-fast\n",
      "──────────────────────────────────────────────────\n",
      "\n",
      "──────────────────────────────────────────────────\n",
      "[LLM_OUTPUT] Raw response from model:\n",
      "```json\n",
      "{\n",
      "  \"justification\": \"The Python_Code_Generator_Tool has already been used to calculate 12 squared, resulting in 144. The Wikipedia_RAG_Search_Tool was then used to find a major historical event in 144 AD but returned no relevant pages. Given the limitations of the Wikipedia_RAG_Search_Tool and the potential for the Generalist_Solution_Generator_Tool to provide more generalized information, the next best step is to utilize the Generalist_Solution_Generator_Tool to find a major historical event in 144 AD, as it may be able to provide a more comprehensive or alternative response.\",\n",
      "  \"context\": \"The year of interest is 144 AD, as calculated from 12 squared. The Generalist_Solution_Generator_Tool should be provided with this year to find a major historical event.\",\n",
      "  \"sub_goal\": \"Find a major historical event that occurred in 144 AD using the Generalist_Solution_Generator_Tool\",\n",
      "  \"tool_name\": \"Generalist_Solution_Generator_Tool\"\n",
      "}\n",
      "```\n",
      "──────────────────────────────────────────────────\n",
      "[LLM Engine] Successfully parsed structured output (Strict).\n",
      "[Planner Justification]: The Python_Code_Generator_Tool has already been used to calculate 12 squared, resulting in 144. The Wikipedia_RAG_Search_Tool was then used to find a major historical event in 144 AD but returned no relevant pages. Given the limitations of the Wikipedia_RAG_Search_Tool and the potential for the Generalist_Solution_Generator_Tool to provide more generalized information, the next best step is to utilize the Generalist_Solution_Generator_Tool to find a major historical event in 144 AD, as it may be able to provide a more comprehensive or alternative response.\n",
      "[Selected Tool]: Generalist_Solution_Generator_Tool\n",
      "[Sub-Goal]: Find a major historical event that occurred in 144 AD using the Generalist_Solution_Generator_Tool\n",
      "\n",
      "==> 📝 Step 3: Generating Command for 'Generalist_Solution_Generator_Tool' (using executor_engine)\n",
      "\n",
      "──────────────────────────────────────────────────\n",
      "[LLM_INPUT] Sending request to model: qwen-1.5-finetuned-grpo-fast\n",
      "──────────────────────────────────────────────────\n",
      "\n",
      "──────────────────────────────────────────────────\n",
      "[LLM_OUTPUT] Raw response from model:\n",
      "```json\n",
      "{\n",
      "  \"analysis\": \"The Generalist_Solution_Generator_Tool requires a 'query' parameter of type 'str'. To find a major historical event in 144 AD, we need to construct a query that includes this year.\",\n",
      "  \"explanation\": \"We will use the calculated year (12 squared = 144) to form a query for the Generalist_Solution_Generator_Tool. The query will ask for a major historical event in 144 AD. We then execute this query using the tool and store the result in the 'execution' variable.\",\n",
      "  \"command\": \"execution = tool.execute(query='What major historical event occurred in 144 AD?')\"\n",
      "}\n",
      "```\n",
      "──────────────────────────────────────────────────\n",
      "[LLM Engine] Successfully parsed structured output (Strict).\n",
      "[Generated Command]:\n",
      "```python\n",
      "execution = tool.execute(query='What major historical event occurred in 144 AD?')\n",
      "```\n",
      "\n",
      "==> 🛠️ Step 3: Executing Command for 'Generalist_Solution_Generator_Tool'\n",
      "\n",
      "──────────────────────────────────────────────────\n",
      "[LLM_INPUT] Sending request to model: qwen-1.5-finetuned-grpo-fast\n",
      "──────────────────────────────────────────────────\n",
      "[LLM ERROR] API Call Failed: Error code: 422 - {'detail': [{'type': 'string_type', 'loc': ['body', 'messages', 1, 'content', 'str'], 'msg': 'Input should be a valid string', 'input': ['What major historical event occurred in 144 AD?']}, {'type': 'model_attributes_type', 'loc': ['body', 'messages', 1, 'content', 'list[union[ChatCompletionContentPartTextParam,ChatCompletionContentPartImageParam,ChatCompletionContentPartVideoParam]]', 0, 'ChatCompletionContentPartTextParam'], 'msg': 'Input should be a valid dictionary or object to extract fields from', 'input': 'What major historical event occurred in 144 AD?'}, {'type': 'model_attributes_type', 'loc': ['body', 'messages', 1, 'content', 'list[union[ChatCompletionContentPartTextParam,ChatCompletionContentPartImageParam,ChatCompletionContentPartVideoParam]]', 0, 'ChatCompletionContentPartImageParam'], 'msg': 'Input should be a valid dictionary or object to extract fields from', 'input': 'What major historical event occurred in 144 AD?'}, {'type': 'model_attributes_type', 'loc': ['body', 'messages', 1, 'content', 'list[union[ChatCompletionContentPartTextParam,ChatCompletionContentPartImageParam,ChatCompletionContentPartVideoParam]]', 0, 'ChatCompletionContentPartVideoParam'], 'msg': 'Input should be a valid dictionary or object to extract fields from', 'input': 'What major historical event occurred in 144 AD?'}]}\n",
      "\n",
      "──────────────────────────────────────────────────\n",
      "[LLM_INPUT] Sending request to model: qwen-1.5-finetuned-grpo-fast\n",
      "──────────────────────────────────────────────────\n",
      "[LLM ERROR] API Call Failed: Error code: 422 - {'detail': [{'type': 'string_type', 'loc': ['body', 'messages', 1, 'content', 'str'], 'msg': 'Input should be a valid string', 'input': ['What major historical event occurred in 144 AD?']}, {'type': 'model_attributes_type', 'loc': ['body', 'messages', 1, 'content', 'list[union[ChatCompletionContentPartTextParam,ChatCompletionContentPartImageParam,ChatCompletionContentPartVideoParam]]', 0, 'ChatCompletionContentPartTextParam'], 'msg': 'Input should be a valid dictionary or object to extract fields from', 'input': 'What major historical event occurred in 144 AD?'}, {'type': 'model_attributes_type', 'loc': ['body', 'messages', 1, 'content', 'list[union[ChatCompletionContentPartTextParam,ChatCompletionContentPartImageParam,ChatCompletionContentPartVideoParam]]', 0, 'ChatCompletionContentPartImageParam'], 'msg': 'Input should be a valid dictionary or object to extract fields from', 'input': 'What major historical event occurred in 144 AD?'}, {'type': 'model_attributes_type', 'loc': ['body', 'messages', 1, 'content', 'list[union[ChatCompletionContentPartTextParam,ChatCompletionContentPartImageParam,ChatCompletionContentPartVideoParam]]', 0, 'ChatCompletionContentPartVideoParam'], 'msg': 'Input should be a valid dictionary or object to extract fields from', 'input': 'What major historical event occurred in 144 AD?'}]}\n",
      "\n",
      "──────────────────────────────────────────────────\n",
      "[LLM_INPUT] Sending request to model: qwen-1.5-finetuned-grpo-fast\n",
      "──────────────────────────────────────────────────\n",
      "[LLM ERROR] API Call Failed: Error code: 422 - {'detail': [{'type': 'string_type', 'loc': ['body', 'messages', 1, 'content', 'str'], 'msg': 'Input should be a valid string', 'input': ['What major historical event occurred in 144 AD?']}, {'type': 'model_attributes_type', 'loc': ['body', 'messages', 1, 'content', 'list[union[ChatCompletionContentPartTextParam,ChatCompletionContentPartImageParam,ChatCompletionContentPartVideoParam]]', 0, 'ChatCompletionContentPartTextParam'], 'msg': 'Input should be a valid dictionary or object to extract fields from', 'input': 'What major historical event occurred in 144 AD?'}, {'type': 'model_attributes_type', 'loc': ['body', 'messages', 1, 'content', 'list[union[ChatCompletionContentPartTextParam,ChatCompletionContentPartImageParam,ChatCompletionContentPartVideoParam]]', 0, 'ChatCompletionContentPartImageParam'], 'msg': 'Input should be a valid dictionary or object to extract fields from', 'input': 'What major historical event occurred in 144 AD?'}, {'type': 'model_attributes_type', 'loc': ['body', 'messages', 1, 'content', 'list[union[ChatCompletionContentPartTextParam,ChatCompletionContentPartImageParam,ChatCompletionContentPartVideoParam]]', 0, 'ChatCompletionContentPartVideoParam'], 'msg': 'Input should be a valid dictionary or object to extract fields from', 'input': 'What major historical event occurred in 144 AD?'}]}\n",
      "[Execution Result]:\n",
      "\"Execution Error: RetryError[<Future at 0x20215c78610 state=finished raised UnprocessableEntityError>]\"\n",
      "\n",
      "==> 🤖 Step 3: Verifying Context (using verifier_engine)\n",
      "\n",
      "──────────────────────────────────────────────────\n",
      "[LLM_INPUT] Sending request to model: qwen-1.5-finetuned-grpo-fast\n",
      "──────────────────────────────────────────────────\n",
      "\n",
      "──────────────────────────────────────────────────\n",
      "[LLM_OUTPUT] Raw response from model:\n",
      "{\"analysis\": \"The query requires calculating 12 squared and then finding a major historical event that occurred in the resulting year AD using Wikipedia. The memory contains the calculation of 12 squared using the Python_Code_Generator_Tool, which resulted in 144. However, the attempt to find a major historical event in 144 AD using the Wikipedia_RAG_Search_Tool did not yield any relevant pages. Additionally, the Generalist_Solution_Generator_Tool failed to provide an answer due to an execution error. Given that the memory does not fully address the query, as it lacks information on a major historical event in 144 AD, and considering that the Ground_Google_Search_Tool could potentially provide the missing information, the analysis concludes that more information is needed.\", \"stop_signal\": false}\n",
      "──────────────────────────────────────────────────\n",
      "[LLM Engine] Successfully parsed structured output (Strict).\n",
      "[Verifier Analysis]: The query requires calculating 12 squared and then finding a major historical event that occurred in the resulting year AD using Wikipedia. The memory contains the calculation of 12 squared using the Python_Code_Generator_Tool, which resulted in 144. However, the attempt to find a major historical event in 144 AD using the Wikipedia_RAG_Search_Tool did not yield any relevant pages. Additionally, the Generalist_Solution_Generator_Tool failed to provide an answer due to an execution error. Given that the memory does not fully address the query, as it lacks information on a major historical event in 144 AD, and considering that the Ground_Google_Search_Tool could potentially provide the missing information, the analysis concludes that more information is needed.\n",
      "[Verifier Conclusion]: CONTINUE\n",
      "\n",
      "================================================================================\n",
      "==> 🎯 Step 4: Planning Next Action (using planner_main_engine)\n",
      "================================================================================\n",
      "\n",
      "──────────────────────────────────────────────────\n",
      "[LLM_INPUT] Sending request to model: qwen-1.5-finetuned-grpo-fast\n",
      "──────────────────────────────────────────────────\n",
      "\n",
      "──────────────────────────────────────────────────\n",
      "[LLM_OUTPUT] Raw response from model:\n",
      "```json\n",
      "{\n",
      "  \"justification\": \"The previous steps have already utilized the Python_Code_Generator_Tool to calculate 12 squared, which resulted in 144. Then, the Wikipedia_RAG_Search_Tool was used to find a major historical event in 144 AD but returned no relevant pages. The Generalist_Solution_Generator_Tool was also attempted for the same purpose but encountered an execution error. Given the task requires finding a historical event and the Wikipedia_RAG_Search_Tool's limitations in the previous step might be due to the specificity or formulation of the query, the Ground_Google_Search_Tool is selected as the next best option. It can provide a broader search capability that might yield more relevant results for historical events in 144 AD.\",\n",
      "  \"context\": \"Year: 144 AD, Previous calculation: 12 squared = 144, Historical event search query: 'major historical events in 144 AD'\",\n",
      "  \"sub_goal\": \"Find a major historical event that occurred in 144 AD using a broader search tool\",\n",
      "  \"tool_name\": \"Ground_Google_Search_Tool\"\n",
      "}\n",
      "```\n",
      "──────────────────────────────────────────────────\n",
      "[LLM Engine] Successfully parsed structured output (Strict).\n",
      "[Planner Justification]: The previous steps have already utilized the Python_Code_Generator_Tool to calculate 12 squared, which resulted in 144. Then, the Wikipedia_RAG_Search_Tool was used to find a major historical event in 144 AD but returned no relevant pages. The Generalist_Solution_Generator_Tool was also attempted for the same purpose but encountered an execution error. Given the task requires finding a historical event and the Wikipedia_RAG_Search_Tool's limitations in the previous step might be due to the specificity or formulation of the query, the Ground_Google_Search_Tool is selected as the next best option. It can provide a broader search capability that might yield more relevant results for historical events in 144 AD.\n",
      "[Selected Tool]: Ground_Google_Search_Tool\n",
      "[Sub-Goal]: Find a major historical event that occurred in 144 AD using a broader search tool\n",
      "\n",
      "==> 📝 Step 4: Generating Command for 'Ground_Google_Search_Tool' (using executor_engine)\n",
      "\n",
      "──────────────────────────────────────────────────\n",
      "[LLM_INPUT] Sending request to model: qwen-1.5-finetuned-grpo-fast\n",
      "──────────────────────────────────────────────────\n",
      "\n",
      "──────────────────────────────────────────────────\n",
      "[LLM_OUTPUT] Raw response from model:\n",
      "```json\n",
      "{\n",
      "  \"analysis\": \"The Ground_Google_Search_Tool requires a query as input, which is a string. The tool's metadata specifies that it is suitable for general information search and is best used for question-type queries. The input query should be 'major historical events in 144 AD' to find the desired historical event.\",\n",
      "  \"explanation\": \"To address the sub-goal, we need to construct a valid Python code that uses the Ground_Google_Search_Tool to find a major historical event in 144 AD. We will use the tool's execute method and assign the result to the 'execution' variable. The query will be 'major historical events in 144 AD'.\",\n",
      "  \"command\": \"execution = tool.execute(query='major historical events in 144 AD')\"\n",
      "}\n",
      "```\n",
      "──────────────────────────────────────────────────\n",
      "[LLM Engine] Successfully parsed structured output (Strict).\n",
      "[Generated Command]:\n",
      "```python\n",
      "execution = tool.execute(query='major historical events in 144 AD')\n",
      "```\n",
      "\n",
      "==> 🛠️ Step 4: Executing Command for 'Ground_Google_Search_Tool'\n",
      "[Execution Result]:\n",
      "\"In 144 AD, several notable historical events unfolded across different parts of the world:\\n\\n**Roman Empire:**\\n*   Lucius Hedius Rufus Lollianus Avitus and Titus Statilius Maximus assumed the role of Roman Consuls.\\n*   The Roman campaigns in Mauretania began during this year.\\n*   The theologian Marcion of Sinope was excommunicated from the church of Rome around 144 AD, leading to the growth of a new sect known as Marcionism. Marcion developed a ditheistic belief system, proposing two gods: a higher, transcendent one and a lower world-creator and ruler. He also published his own canon of Christian scriptures, which included ten Pauline epistles and an edited version of the Gospel of Luke, notably excluding the Old Testament.\\n\\n**Asia:**\\n*   In the Chinese Han dynasty, there was a change in the era name from Hanan (3rd year) to Jiankang.\\n*   Emperor Han Shundi was succeeded by his infant son, Han Chongdi, in the Han dynasty.\\n*   In Vietnam, during the second Era of Northern Domination, a rebellion by the Cham people erupted in Jiaozhi and Jiuzhen, though the rebels later surrendered to the governor of Jiaozhi in the same year.\\n*   The reign of Huvishka as the emperor of the Kushan Empire also occurred around this time.\"\n",
      "\n",
      "==> 🤖 Step 4: Verifying Context (using verifier_engine)\n",
      "\n",
      "──────────────────────────────────────────────────\n",
      "[LLM_INPUT] Sending request to model: qwen-1.5-finetuned-grpo-fast\n",
      "──────────────────────────────────────────────────\n",
      "\n",
      "──────────────────────────────────────────────────\n",
      "[LLM_OUTPUT] Raw response from model:\n",
      "{\"analysis\": \"The query requires calculating 12 squared and then finding a major historical event that occurred in the resulting year AD using Wikipedia. The memory contains the calculation of 12 squared, which equals 144, and the results of using the Wikipedia_RAG_Search_Tool, Generalist_Solution_Generator_Tool, and Ground_Google_Search_Tool to find historical events in 144 AD. The Ground_Google_Search_Tool provided several notable historical events that occurred in 144 AD, including the Roman campaigns in Mauretania, the excommunication of Marcion of Sinope, and changes in the Chinese Han dynasty. The memory fully addresses all parts of the query, providing both the calculation result and historical events for the year 144 AD.\", \"stop_signal\": true}\n",
      "──────────────────────────────────────────────────\n",
      "[LLM Engine] Successfully parsed structured output (Strict).\n",
      "[Verifier Analysis]: The query requires calculating 12 squared and then finding a major historical event that occurred in the resulting year AD using Wikipedia. The memory contains the calculation of 12 squared, which equals 144, and the results of using the Wikipedia_RAG_Search_Tool, Generalist_Solution_Generator_Tool, and Ground_Google_Search_Tool to find historical events in 144 AD. The Ground_Google_Search_Tool provided several notable historical events that occurred in 144 AD, including the Roman campaigns in Mauretania, the excommunication of Marcion of Sinope, and changes in the Chinese Han dynasty. The memory fully addresses all parts of the query, providing both the calculation result and historical events for the year 144 AD.\n",
      "[Verifier Conclusion]: STOP\n",
      "\n",
      "================================================================================\n",
      "==> 🐙 Generating Final Output (using planner_fixed_engine)\n",
      "================================================================================\n",
      "\n",
      "──────────────────────────────────────────────────\n",
      "[LLM_INPUT] Sending request to model: qwen-1.5-finetuned-grpo-fast\n",
      "──────────────────────────────────────────────────\n",
      "\n",
      "──────────────────────────────────────────────────\n",
      "[LLM_OUTPUT] Raw response from model:\n",
      "## Step 1: Review the Query and Results\n",
      "The query requires calculating 12 squared and then finding a major historical event that occurred in the resulting year AD using Wikipedia. The results from the actions include the calculation of 12 squared, which equals 144, and the search for historical events in 144 AD using various tools.\n",
      "\n",
      "## Step 2: Synthesize Key Findings\n",
      "The key findings include the successful calculation of 12 squared as 144. For the historical event search, initial attempts using the Wikipedia_RAG_Search_Tool and Generalist_Solution_Generator_Tool were unsuccessful. However, the Ground_Google_Search_Tool provided detailed information on major historical events in 144 AD, including changes in the Roman Empire, the Chinese Han dynasty, and other parts of Asia.\n",
      "\n",
      "## Step 3: Provide a Direct Answer\n",
      "Based on the calculation and historical event search, the direct answer to the query involves both the result of 12 squared and a significant historical event in 144 AD.\n",
      "\n",
      "The final answer is: $\\boxed{144, Major historical events in 144 AD included the Roman campaigns in Mauretania, the excommunication of Marcion of Sinope, and changes in the Chinese Han dynasty.}$\n",
      "──────────────────────────────────────────────────\n",
      "\n",
      "================================================================================\n",
      "✅ FINAL SYNTHESIZED ANSWER\n",
      "================================================================================\n",
      "## Step 1: Review the Query and Results\n",
      "The query requires calculating 12 squared and then finding a major historical event that occurred in the resulting year AD using Wikipedia. The results from the actions include the calculation of 12 squared, which equals 144, and the search for historical events in 144 AD using various tools.\n",
      "\n",
      "## Step 2: Synthesize Key Findings\n",
      "The key findings include the successful calculation of 12 squared as 144. For the historical event search, initial attempts using the Wikipedia_RAG_Search_Tool and Generalist_Solution_Generator_Tool were unsuccessful. However, the Ground_Google_Search_Tool provided detailed information on major historical events in 144 AD, including changes in the Roman Empire, the Chinese Han dynasty, and other parts of Asia.\n",
      "\n",
      "## Step 3: Provide a Direct Answer\n",
      "Based on the calculation and historical event search, the direct answer to the query involves both the result of 12 squared and a significant historical event in 144 AD.\n",
      "\n",
      "The final answer is: $\\boxed{144, Major historical events in 144 AD included the Roman campaigns in Mauretania, the excommunication of Marcion of Sinope, and changes in the Chinese Han dynasty.}$\n"
     ]
    }
   ],
   "source": [
    "def construct_solver():\n",
    "    \"\"\"Configures and constructs an instance of the Solver agent.\"\"\"\n",
    "    # Define which LLM model to use for each agent role.\n",
    "    planner_main_engine, planner_fixed_engine, verifier_engine, executor_engine = MODEL_NAME, MODEL_NAME, MODEL_NAME, MODEL_NAME\n",
    "    \n",
    "    # Define the list of tools the agent should have access to.\n",
    "    enabled_tools = [\"Base_Generator_Tool\", \"Python_Coder_Tool\", \"Google_Search_Tool\", \"Wikipedia_RAG_Search_Tool\"]\n",
    "    \n",
    "    # Define the specific LLM engine for each tool. 'Default' means the tool doesn't need an LLM.\n",
    "    tool_engine = [MODEL_NAME, MODEL_NAME, \"Default\", MODEL_NAME]\n",
    "    \n",
    "    return Solver(planner_main_engine, planner_fixed_engine, verifier_engine, executor_engine, enabled_tools, tool_engine, max_steps=5)\n",
    "\n",
    "# ===================\n",
    "# RUN THE SOLVER\n",
    "# ===================\n",
    "\n",
    "# 1. Create the solver instance based on our configuration.\n",
    "solver = construct_solver()\n",
    "\n",
    "# 2. Define the complex, multi-step query we want the agent to solve.\n",
    "query_to_solve = \"Calculate 12 squared, and then use Wikipedia to find out what major historical event happened in that year (AD).\"\n",
    "\n",
    "# 3. Call the .solve() method to start the agentic workflow.\n",
    "final_answer = solver.solve(query_to_solve)\n",
    "\n",
    "# 4. Print the final, synthesized answer produced by the agent.\n",
    "print(f\"\\n{'='*80}\\n✅ FINAL SYNTHESIZED ANSWER\\n{'='*80}\\n{final_answer}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv-agentic-flow-final (3.10.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
