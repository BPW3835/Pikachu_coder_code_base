{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro-1",
   "metadata": {},
   "source": [
    "# 03. In-the-Flow Agentic System Optimization with Flow-GRPO\n",
    "\n",
    "Welcome to the third notebook, which dives deep into the training phase of an agentic system. This script implements the **Flow-GRPO (Group Rollout Policy Optimization)** algorithm to fine-tune the agent's core decision-making module, the **Planner**.\n",
    "\n",
    "## Why Flow-GRPO?\n",
    "\n",
    "Traditional Supervised Fine-Tuning (SFT) trains a model to mimic expert trajectories (Input $\\rightarrow$ Optimal Action). However, SFT does not account for the quality of the *outcome* in a real environment. If the expert makes a mistake early on, the model learns the mistake.\n",
    "\n",
    "Reinforcement Learning (RL) addresses this by rewarding the agent based on the *final success* of its entire trajectory. Flow-GRPO is a custom RL algorithm derived from **Proximal Policy Optimization (PPO)**, tailored for multi-step, agentic tasks:\n",
    "\n",
    "1.  **On-Policy Learning:** The agent learns directly from its own generated experience (trajectories).\n",
    "2.  **Group Rollouts (The GRPO part):** For a single input query, the agent generates $N$ diverse trajectories ($N$ = `rollout_n`). This group of outcomes allows us to compute a **relative advantage** (or counterfactual reward) for each trajectory. Instead of a trajectory simply being \"good\" or \"bad,\" it is judged relative to the other outcomes in its group.\n",
    "3.  **Stability (The PPO part):** It uses the PPO objective, which constrains the policy update size (via the clipping mechanism, $\\epsilon$), preventing the new policy from deviating too radically from the old policy, ensuring stable training.\n",
    "4.  **Reward from External Judge (RLHF):** The reward signal comes from a powerful, fixed \"Judge LLM\" (like GPT-4o) which acts as an impartial evaluator, providing a nuanced, human-aligned reward for the final answer quality.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "config-1",
   "metadata": {},
   "source": [
    "## Section 1: Configuration and Dependencies\n",
    "\n",
    "We start by importing all necessary Python modules, setting up the hardware device, and defining the global hyperparameters and model paths via a configuration dataclass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "grpo-imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# Dependencies\n",
    "# ==============================================================================\n",
    "# !pip install transformers peft datasets json_repair tqdm\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "from typing import List, Dict, Any, Tuple, Optional\n",
    "from dataclasses import dataclass, field\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from datasets import load_dataset\n",
    "from torch.optim import AdamW\n",
    "\n",
    "# Transformers & PEFT for efficient training\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM, \n",
    "    AutoTokenizer, \n",
    "    BitsAndBytesConfig, \n",
    "    get_scheduler\n",
    ")\n",
    "from peft import (\n",
    "    LoraConfig, \n",
    "    get_peft_model, \n",
    "    prepare_model_for_kbit_training, \n",
    "    PeftModel\n",
    ")\n",
    "\n",
    "# Library for robust JSON parsing from LLM outputs\n",
    "import json_repair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45dd535b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# IMPORT CUSTOM MODULES (Assumed to be in utils.py)\n",
    "# ==============================================================================\n",
    "from utils import (\n",
    "    BaseTool, QueryAnalysis, NextStep, ToolCommand, MemoryVerification, \n",
    "    Select_Relevant_Queries, Base_Generator_Tool, Python_Coder_Tool, \n",
    "    Google_Search_Tool, Wikipedia_Search_Tool, Memory, create_llm_engine, \n",
    "    make_json_serializable_truncated, AnswerVerification, EngineLM, ChatVLLM\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "config-2",
   "metadata": {},
   "source": [
    "### 1.1 Training Configuration\n",
    "\n",
    "The `TrainingConfig` dataclass holds all critical hyperparameters. Note the careful selection of models: a smaller Qwen 1.5B model is used for the *trainable Policy/Planner*, while a more powerful model (Qwen 2.5B, potentially hosted externally) is used for the *Fixed Environment* (Executor/Verifier) and the *Reward Judge* (GPT-4o). This separation is vital: we train the smaller model to make optimal decisions, but we trust the larger models to simulate the complex environment and judge performance accurately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "grpo-config",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "@dataclass\n",
    "class TrainingConfig:\n",
    "    \"\"\"Global configuration for the training run, using Python's dataclass for structured setup.\"\"\"\n",
    "    \n",
    "    # --- Data Config ---\n",
    "    data_file: str = \"./data/train/combined_train.parquet\" # Input path for the combined training data.\n",
    "\n",
    "    # --- Model Config ---\n",
    "    base_model_name: str = \"Qwen/Qwen2-1.5B-Instruct\" # The model being trained (the Policy/Planner).\n",
    "    fixed_model_name: str = \"Qwen/Qwen2.5-7B-Instruct\" # The powerful, fixed model for Execution/Verification.\n",
    "    fixed_model_api_base: str = \"http://localhost:8001/v1\" # Endpoint for the fixed model (assumes a vLLM server).\n",
    "    \n",
    "    # --- Training Hyperparameters ---\n",
    "    run_name: str = \"flow_grpo_training_run_v1\"\n",
    "    output_dir: str = \"./agentflow_checkpoints\" # Directory to save checkpoints.\n",
    "    learning_rate: float = 1e-6\n",
    "    train_batch_size: int = 2 # Number of unique queries processed per optimization loop.\n",
    "    rollout_n: int = 4 # N: Number of trajectories generated per unique query (GRPO group size).\n",
    "    gradient_accumulation_steps: int = 4 # Accumulate gradients over this many effective steps before updating weights.\n",
    "    num_train_epochs: int = 1\n",
    "    \n",
    "    # --- GRPO/PPO Hyperparameters ---\n",
    "    ppo_clip_eps: float = 0.2  # PPO Clipping range (e.g., 20%). Prevents drastic policy updates.\n",
    "    kl_coef: float = 0.01      # Coefficient for the KL-Divergence penalty (KL regularization).\n",
    "    max_grad_norm: float = 1.0 # Gradient clipping value.\n",
    "    \n",
    "    # --- Agent Execution Config ---\n",
    "    max_turns: int = 5         # Max steps the agent can take for a single query (trajectory length limit).\n",
    "    max_seq_length: int = 4096 # Context window limit for the base model.\n",
    "    \n",
    "    # --- Tools Config ---\n",
    "    # The list of tools the agent can use.\n",
    "    enabled_tools: List[str] = field(default_factory=lambda: [\"Python_Coder_Tool\", \"Wikipedia_RAG_Search_Tool\"])\n",
    "    # The engine used by each tool instance (can be different from the Policy model).\n",
    "    tool_engine: List[str] = field(default_factory=lambda: [\"gpt-4o-mini\", \"gpt-4o-mini\"])\n",
    "    \n",
    "    # --- Reward Config ---\n",
    "    reward_model_name: str = \"gpt-4o\" # The high-quality model used as the impartial Judge.\n",
    "\n",
    "# Initialize Config\n",
    "config = TrainingConfig()\n",
    "os.makedirs(config.output_dir, exist_ok=True) # Ensure output directory exists.\n",
    "\n",
    "# Set Device (prioritize GPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data-struct-1",
   "metadata": {},
   "source": [
    "## Section 2: Data Structures for Trajectories\n",
    "\n",
    "In RL, every step of interaction needs to be recorded accurately to compute the policy gradient. The `TurnData` dataclass captures the essential information generated by the *Policy Model* (the Planner) during a single step (or *turn*) of the agent's multi-step decision process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "grpo-turndata",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TurnData:\n",
    "    \"\"\"Stores data for a single step (turn) in a trajectory for training.\"\"\"\n",
    "    prompt_str: str              # The input prompt (current state) given to the Planner LLM.\n",
    "    action_str: str              # The LLM's full output (the action plan).\n",
    "    prompt_ids: torch.Tensor     # Tokenized version of the prompt.\n",
    "    action_ids: torch.Tensor     # Tokenized version of the action.\n",
    "    # CRITICAL: The log likelihood of the action tokens under the *current* Policy model.\n",
    "    # This is $log(\\pi_{old}(a|s))$ in the PPO formulation.\n",
    "    action_log_probs: torch.Tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "model-init-1",
   "metadata": {},
   "source": [
    "## Section 3: Model Initialization (QLoRA & PEFT)\n",
    "\n",
    "We initialize the core components of our training system:\n",
    "\n",
    "1.  **Tokenizer:** Essential for converting text prompts to tokens and back.\n",
    "2.  **Policy Model (`policy_model`):** The model we are training. We use **QLoRA (Quantized Low-Rank Adaptation)** to load it in 4-bit precision, drastically reducing VRAM usage, while using **PEFT (Parameter-Efficient Fine-Tuning)** to attach LoRA adapters, allowing us to train only a small fraction of the model's parameters.\n",
    "3.  **Reference Model (`ref_model`):** In PPO/GRPO, the previous policy is needed to compute the importance ratio. Here, we initially set the reference model equal to the policy model, using a context manager (`disable_adapter()`) later to compute the reference log probabilities without the influence of the current LoRA weights.\n",
    "4.  **Fixed External LLMs:** We initialize the robust, external LLM clients required for execution/verification and reward computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "grpo-model-init",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 2. Initializing Models ---\n",
      "--> Loading Tokenizer...\n",
      "--> Loading Trainable Planner Model (Qwen/Qwen2-1.5B-Instruct)...\n",
      "trainable params: 16,777,216 || all params: 1,518,804,992 || trainable%: 1.1046\n",
      "--> Initializing Fixed LLM Engines (Executor, Verifier, Reward)...\n",
      "   âœ… Fixed LLM and Reward LLM connections successful.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- 2. Initializing Models ---\")\n",
    "\n",
    "print(\"--> Loading Tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.base_model_name, trust_remote_code=True)\n",
    "# Ensure padding token exists and set padding side to left (standard for generation/decoding).\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"left\"\n",
    "\n",
    "print(f\"--> Loading Trainable Planner Model ({config.base_model_name})...\")\n",
    "# Load model in 4-bit using BitsAndBytesConfig (QLoRA).\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True, \n",
    "    bnb_4bit_quant_type=\"nf4\", # Normalized Float 4-bit quantization.\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16 # Use bfloat16 for computation.\n",
    ")\n",
    "\n",
    "policy_model = AutoModelForCausalLM.from_pretrained(\n",
    "    config.base_model_name, \n",
    "    quantization_config=bnb_config, \n",
    "    device_map=\"auto\", # Automatically distributes the model across available GPUs.\n",
    "    trust_remote_code=True, \n",
    "    use_cache=False # Disable cache for gradient checkpointing during training.\n",
    ")\n",
    "\n",
    "# Prepare model for k-bit training and define LoRA configuration.\n",
    "policy_model = prepare_model_for_kbit_training(policy_model)\n",
    "peft_config = LoraConfig(\n",
    "    r=16, \n",
    "    lora_alpha=32, \n",
    "    # Target all major projection layers for optimal performance.\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"], \n",
    "    lora_dropout=0.05, \n",
    "    bias=\"none\", \n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "policy_model = get_peft_model(policy_model, peft_config)\n",
    "policy_model.print_trainable_parameters()\n",
    "\n",
    "# The reference model starts identical to the policy model.\n",
    "ref_model = policy_model \n",
    "\n",
    "print(\"--> Initializing Fixed LLM Engines (Executor, Verifier, Reward)...\")\n",
    "try:\n",
    "    # Initialize the fixed LLM for executing tool commands and verification logic.\n",
    "    fixed_llm = create_llm_engine(config.fixed_model_name, base_url=config.fixed_model_api_base, temperature=0.0)\n",
    "    # Initialize the reward LLM (Judge).\n",
    "    reward_llm = create_llm_engine(config.reward_model_name, temperature=0.0)\n",
    "    \n",
    "    # Test connections to external APIs/servers.\n",
    "    fixed_llm.generate(\"Ping\")\n",
    "    reward_llm.generate(\"Ping\")\n",
    "    print(\"   âœ… Fixed LLM and Reward LLM connections successful.\")\n",
    "except Exception as e:\n",
    "    # Halt execution if critical external components are unreachable.\n",
    "    raise ConnectionError(f\"Could not connect to one of the LLM endpoints. Ensure servers are running. Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "agentic-system-1",
   "metadata": {},
   "source": [
    "## Section 4: The Agentic System (Environment and Policy Interaction)\n",
    "\n",
    "The `AgenticSystem` class simulates the environment where the Planner Policy operates. It encapsulates the core components necessary for a single training rollout:\n",
    "\n",
    "1.  **Tool Management:** Loads and provides access to the specialized tools.\n",
    "2.  **State Generation:** Formulates the prompt (State $S_t$) for the Planner based on the query and memory.\n",
    "3.  **Action Generation & Log Prob Calculation:** Uses the Policy Model to generate the next action and captures the log probability of that action, which is essential for the PPO objective."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "agentic-system-2",
   "metadata": {},
   "source": [
    "### 4.1 Agentic System Initialization and Tool Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "grpo-agent-init",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 3. Setting up Agentic System for Rollouts ---\n",
      "--> Loading Agent Tools...\n",
      "    - Loading 'Python_Coder_Tool' with engine 'gpt-4o-mini'\n",
      "    - Loading 'Wikipedia_RAG_Search_Tool' with engine 'gpt-4o-mini'\n",
      "   âœ… Tools loaded.\n"
     ]
    }
   ],
   "source": [
    "class AgenticSystem:\n",
    "    \"\"\"Manages the interaction between the Policy, the Tools, and the Fixed LLM Environment.\"\"\"\n",
    "    def __init__(self, policy_model, tokenizer, fixed_llm):\n",
    "        self.policy_model = policy_model # The trainable model.\n",
    "        self.tokenizer = tokenizer\n",
    "        self.fixed_llm = fixed_llm # The external Executor/Verifier model.\n",
    "        self.tools_map = self._load_tools() # Dictionary of active tool instances.\n",
    "        self.memory = None # Agent's memory instance, reset per trajectory.\n",
    "\n",
    "    def _load_tools(self) -> Dict[str, BaseTool]:\n",
    "        \"\"\"Initializes the tools specified in the global configuration.\"\"\"\n",
    "        print(\"--> Loading Agent Tools...\")\n",
    "        tools = {}\n",
    "        # Mapping tool names to their respective classes from utils.py.\n",
    "        tool_classes = {\n",
    "            \"Python_Coder_Tool\": Python_Coder_Tool, \n",
    "            \"Wikipedia_RAG_Search_Tool\": Wikipedia_Search_Tool, \n",
    "            \"Base_Generator_Tool\": Base_Generator_Tool\n",
    "        }\n",
    "        for i, name in enumerate(config.enabled_tools):\n",
    "            engine = config.tool_engine[i]\n",
    "            if name in tool_classes:\n",
    "                print(f\"    - Loading '{name}' with engine '{engine}'\")\n",
    "                # Instantiate the tool, passing the required engine name.\n",
    "                tools[name] = tool_classes[name](model_string=engine)\n",
    "        print(\"   âœ… Tools loaded.\")\n",
    "        return tools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "agentic-system-3",
   "metadata": {},
   "source": [
    "### 4.2 State/Prompt Construction\n",
    "\n",
    "This method takes the current context (query and memory) and formats it into a cohesive prompt. This prompt represents the current **State ($S_t$)** observed by the Policy model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "grpo-build-prompt",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_planner_prompt(self, question, available_tools, memory_actions):\n",
    "    \"\"\"Constructs the state prompt for the Planner model, providing all relevant context.\"\"\"\n",
    "    return f\"\"\"Task: Determine the optimal next step to address the query.\n",
    "\n",
    "Context:\n",
    "- Query: {question}\n",
    "- Available Tools: {json.dumps(available_tools)} # List of tools for the Planner to choose from.\n",
    "- Previous Steps: {json.dumps(memory_actions)} # The history (memory) of executed actions.\n",
    "\n",
    "Response Format:\n",
    "1. Justification: ...\n",
    "2. Context: ...\n",
    "3. Sub-Goal: ...\n",
    "4. Tool Name: ...\n",
    "\n",
    "Response:\"\"\" # The Planner continues the prompt from here, generating the action.\n",
    "\n",
    "# Attaching the method to the class dynamically.\n",
    "AgenticSystem.build_planner_prompt = build_planner_prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "agentic-system-4",
   "metadata": {},
   "source": [
    "### 4.3 Action Generation and Log Probability Calculation\n",
    "\n",
    "This is arguably the most complex part of the Policy rollout. For RL training, we need two things from the Policy model: the generated action (the text plan) and the precise **log probability** of generating that sequence of tokens. This log probability ($\\log \\pi(a|s)$) is the foundation of the PPO importance ratio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "grpo-generate-action",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def generate_planner_action(self, prompt_str: str) -> Tuple[str, torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"Generates a thought/action plan from the policy model and computes log probabilities.\"\"\"\n",
    "    self.policy_model.eval() # Policy generation is done in evaluation mode.\n",
    "    inputs = self.tokenizer(prompt_str, return_tensors=\"pt\", truncation=True, max_length=config.max_seq_length).to(device)\n",
    "    \n",
    "    # Generate with sampling to allow exploration and diverse trajectories (crucial for GRPO).\n",
    "    outputs = self.policy_model.generate(\n",
    "        **inputs, \n",
    "        max_new_tokens=512, \n",
    "        do_sample=True, \n",
    "        temperature=0.7, # Higher temperature for exploration.\n",
    "        top_p=0.9, \n",
    "        pad_token_id=self.tokenizer.eos_token_id, \n",
    "        output_scores=True, # MUST be True to get the logits (scores) for log prob calculation.\n",
    "        return_dict_in_generate=True\n",
    "    )\n",
    "    \n",
    "    # Extract sequences (only the generated part, excluding the input prompt).\n",
    "    generated_ids = outputs.sequences[0, inputs.input_ids.shape[1]:]\n",
    "    generated_text = self.tokenizer.decode(generated_ids, skip_special_tokens=True)\n",
    "    \n",
    "    # Compute Log Probs from the raw scores (logits).\n",
    "    # 1. Stack scores: (num_generated_tokens x 1 x vocab_size) -> (1 x num_generated_tokens x vocab_size).\n",
    "    all_logits = torch.stack(outputs.scores, dim=1) \n",
    "    # 2. Convert logits to log probabilities using log_softmax.\n",
    "    log_probs = F.log_softmax(all_logits, dim=-1)\n",
    "    \n",
    "    # 3. Gather the log probs corresponding to the specific tokens the model actually chose.\n",
    "    # generated_ids: [seq_len] -> unsqueeze to [1, seq_len, 1] for torch.gather.\n",
    "    action_log_probs = log_probs.gather(2, generated_ids.unsqueeze(0).unsqueeze(-1)).squeeze(-1).squeeze(0)\n",
    "    \n",
    "    # Return action text, token IDs, and their log probabilities (moved to CPU).\n",
    "    return generated_text, generated_ids.cpu(), action_log_probs.cpu()\n",
    "\n",
    "AgenticSystem.generate_planner_action = generate_planner_action"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "agentic-system-5",
   "metadata": {},
   "source": [
    "### 4.4 Execution and Verification Logic\n",
    "\n",
    "The policy only generates a *plan* (the action $A_t$). The environment must carry out that plan (Execution) and determine if the agent should continue (Verification). This task is delegated to the powerful, fixed LLM to ensure reliable tool usage and reflection, decoupling it from the trainable Policy model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "grpo-executor-verifier",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_executor_verifier(self, query: str, plan: NextStep) -> Tuple[str, str, str]:\n",
    "    \"\"\"Executes the chosen tool and uses the Fixed LLM to verify the result.\"\"\"\n",
    "    command_used, tool_output = \"N/A\", f\"Error: Tool '{plan.tool_name}' not found.\"\n",
    "    \n",
    "    # 1. Execute Tool\n",
    "    if plan.tool_name in self.tools_map:\n",
    "        tool = self.tools_map[plan.tool_name]\n",
    "        # Prompt the fixed LLM (Executor) to write the exact Python command.\n",
    "        executor_prompt = f\"\"\"Task: Generate a precise command to execute the selected tool.\n",
    "\n",
    "            Context:\n",
    "            - **Query:** {query}\n",
    "            - **Sub-Goal:** {plan.sub_goal}\n",
    "            - **Tool Name:** {plan.tool_name}\n",
    "            - **Relevant Data:** {plan.context}\n",
    "\n",
    "            Instructions: Construct valid Python code to call `tool.execute()` with the correct arguments to achieve the sub-goal. Assign the result to a variable named `execution`. Output only the code wrapped in ```python```.\"\"\"\n",
    "        try:\n",
    "            # Use the fixed LLM to generate the structured tool command.\n",
    "            command_response = self.fixed_llm.generate(executor_prompt, response_format=ToolCommand)\n",
    "            command_used = command_response.command\n",
    "            \n",
    "            # Safe execution environment: `exec` runs the generated command.\n",
    "            local_scope = {'tool': tool}\n",
    "            exec(command_used, {}, local_scope)\n",
    "            tool_output = local_scope.get('execution', \"Error: 'execution' variable not found.\")\n",
    "        except Exception as e: \n",
    "            tool_output = f\"Execution failed: {e}\"\n",
    "    \n",
    "    # 2. Verify Result (using the Fixed LLM as the Verifier)\n",
    "    verifier_prompt = f\"\"\"Task: Evaluate if the current memory is complete enough to answer the query.\n",
    "\n",
    "        Context:\n",
    "        - Query: {query}\n",
    "        - Memory: {json.dumps(self.memory.get_actions(), indent=2)}\n",
    "        - Latest Action Result: {tool_output}\n",
    "\n",
    "        Instructions: Is the query fully answered? Conclude your analysis with \"Conclusion: STOP\" or \"Conclusion: CONTINUE\".\"\"\"\n",
    "    \n",
    "    # Get the verification decision from the Fixed LLM.\n",
    "    verify_resp = self.fixed_llm.generate(verifier_prompt)\n",
    "    # Store the output in a truncated, serializable format for memory.\n",
    "    return command_used, make_json_serializable_truncated(tool_output), verify_resp\n",
    "\n",
    "AgenticSystem.run_executor_verifier = run_executor_verifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "agentic-system-6",
   "metadata": {},
   "source": [
    "### 4.5 Full Trajectory Rollout\n",
    "\n",
    "This method orchestrates the entire agentic process for one input query. It loops through planning, execution, and verification, collecting all the necessary `TurnData` records (State, Action, Log Prob) until the task is marked as complete or `max_turns` is reached. The collected data forms a single trajectory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "grpo-run-trajectory",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_trajectory(self, query: str) -> Tuple[List[TurnData], str]:\n",
    "    \"\"\"Runs a full multi-step rollout for a single query, collecting TurnData.\"\"\"\n",
    "    self.memory = Memory() # Start with fresh memory.\n",
    "    turns_data = []\n",
    "    final_answer = \"No answer generated.\"\n",
    "    \n",
    "    for t in range(config.max_turns):\n",
    "        # 1. Plan (Policy Action)\n",
    "        planner_prompt = self.build_planner_prompt(query, list(self.tools_map.keys()), self.memory.get_actions())\n",
    "        action_text, action_ids, action_log_probs = self.generate_planner_action(planner_prompt)\n",
    "        \n",
    "        # 2. Parse Action\n",
    "        try: \n",
    "            # Robustly load the structured plan from the Policy model's output.\n",
    "            plan = NextStep(**json.loads(json_repair.loads(action_text)))\n",
    "        except Exception:\n",
    "            # Fail gracefully if parsing fails, forcing an early stop/self-answer attempt.\n",
    "            plan = NextStep(justification=\"Parse failed\", context=\"\", sub_goal=\"Final Answer\", tool_name=\"None\")\n",
    "        \n",
    "        # Check for self-determined stop (i.e., the Policy believes it has the answer).\n",
    "        if \"final answer\" in plan.sub_goal.lower() or plan.tool_name.lower() == \"none\":\n",
    "            final_answer = plan.context\n",
    "            # Store this last turn data.\n",
    "            turns_data.append(TurnData(\n",
    "                prompt_str=planner_prompt, action_str=action_text, \n",
    "                prompt_ids=self.tokenizer(planner_prompt, return_tensors=\"pt\").input_ids[0], \n",
    "                action_ids=action_ids, action_log_probs=action_log_probs\n",
    "            ))\n",
    "            break\n",
    "        \n",
    "        # 3. Execute & Verify (Environment Interaction)\n",
    "        command_used, tool_output, verify_decision = self.run_executor_verifier(query, plan)\n",
    "        \n",
    "        # 4. Update Memory\n",
    "        self.memory.add_action(t, plan.tool_name, plan.sub_goal, command_used, tool_output)\n",
    "        \n",
    "        # 5. Store Turn Data for Training\n",
    "        turns_data.append(TurnData(\n",
    "            prompt_str=planner_prompt, action_str=action_text, \n",
    "            prompt_ids=self.tokenizer(planner_prompt, return_tensors=\"pt\").input_ids[0], \n",
    "            action_ids=action_ids, action_log_probs=action_log_probs\n",
    "        ))\n",
    "        \n",
    "        # 6. Check Verifier Stop (Environment signal to stop)\n",
    "        if \"STOP\" in verify_decision.upper():\n",
    "            # If the Verifier stops, use the Fixed LLM to generate the best possible final answer based on memory.\n",
    "            generator_prompt = f\"Based on this history, what is the final answer to the query '{query}'?\\n\\nHistory:\\n{json.dumps(self.memory.get_actions(), indent=2)}\"\n",
    "            final_answer = self.fixed_llm.generate(generator_prompt)\n",
    "            break\n",
    "    else:\n",
    "        # If max turns reached without a stop signal.\n",
    "        final_answer = \"Max turns reached.\"\n",
    "        \n",
    "    return turns_data, final_answer\n",
    "\n",
    "AgenticSystem.run_trajectory = run_trajectory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reward-loss-1",
   "metadata": {},
   "source": [
    "## Section 5: Reward and Loss Functions\n",
    "\n",
    "In RL, the Policy is updated by minimizing a loss function derived from the rewards. Here, we define the mechanism to assign rewards and the PPO-based objective function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reward-loss-2",
   "metadata": {},
   "source": [
    "### 5.1 Reward Calculation (RLHF Judge)\n",
    "\n",
    "We use an external, powerful LLM (`gpt-4o`) as a judge to determine if the final answer matches the ground truth. This provides a human-quality assessment of correctness, yielding a simple binary reward (1.0 for success, 0.0 for failure) for the entire trajectory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "grpo-reward",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_reward(query: str, ground_truth: str, final_answer: str) -> float:\n",
    "    \"\"\"Computes a binary reward (1.0 or 0.0) using the Judge LLM.\"\"\"\n",
    "    prompt = f\"\"\"You are an impartial judge. Evaluate if the model's answer correctly addresses the query based on the ground truth.\n",
    "\n",
    "Query: {query}\n",
    "Ground Truth Answer: {ground_truth}\n",
    "Model's Final Answer: {final_answer}\n",
    "\n",
    "Is the model's answer correct?\"\"\"\n",
    "    try:\n",
    "        # Use the Judge LLM to determine correctness, forcing structured output.\n",
    "        judgement = reward_llm.generate(prompt, response_format=AnswerVerification)\n",
    "        return 1.0 if judgement.true_false else 0.0\n",
    "    except Exception: \n",
    "        # Fallback: simple string match if the Judge LLM API call or parsing fails.\n",
    "        return 1.0 if str(ground_truth).lower() in str(final_answer).lower() else 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reward-loss-3",
   "metadata": {},
   "source": [
    "### 5.2 PPO Loss Function (The Flow-GRPO Objective)\n",
    "\n",
    "The `compute_ppo_loss` function implements the core optimization objective. It takes trajectories and pre-calculated **Advantages** (the GRPO signal) and computes the PPO loss, which consists of two main terms:\n",
    "\n",
    "1.  **Clipped Surrogate Loss:** Ensures policy updates move in the direction of higher reward while remaining close to the reference policy (clipping parameter $\\epsilon$).\n",
    "2.  **KL Divergence Penalty:** A regularizer ($\\text{KL\\_coef}$) that prevents the policy from diverging too far from the reference model, ensuring training stability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "grpo-ppo-loss",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_ppo_loss(\n",
    "    policy_model: PeftModel, \n",
    "    ref_model: PeftModel, \n",
    "    tokenizer: AutoTokenizer, \n",
    "    trajectories: List[List[TurnData]], # A batch of trajectories.\n",
    "    advantages: torch.Tensor # The GRPO advantage computed for each trajectory.\n",
    ") -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"Computes the PPO/GRPO loss for a batch of trajectories.\"\"\"\n",
    "    total_policy_loss = torch.tensor(0.0, device=device)\n",
    "    total_kl_div = torch.tensor(0.0, device=device)\n",
    "    valid_trajectories = 0\n",
    "\n",
    "    for i, trajectory in enumerate(trajectories):\n",
    "        if not trajectory: continue\n",
    "        \n",
    "        # --- Data Preparation for Batching ---\n",
    "        # The model needs the full sequence (Prompt + Action) to calculate log probabilities correctly.\n",
    "        full_input_ids_list = [trajectory[0].prompt_ids]\n",
    "        # Labels are masked. We set labels for Prompt tokens to -100 (ignored in loss).\n",
    "        full_labels_list = [torch.full_like(trajectory[0].prompt_ids, -100)]\n",
    "        \n",
    "        for turn in trajectory:\n",
    "            full_input_ids_list.append(turn.action_ids)\n",
    "            full_labels_list.append(turn.action_ids) # Labels for Action tokens are the tokens themselves.\n",
    "            \n",
    "        input_ids = torch.cat(full_input_ids_list, dim=-1).to(device)\n",
    "        labels = torch.cat(full_labels_list, dim=-1).to(device)\n",
    "        \n",
    "        # --- Policy Log Probs (New Policy) ---\n",
    "        outputs = policy_model(input_ids=input_ids.unsqueeze(0), labels=labels.unsqueeze(0))\n",
    "        # HuggingFace loss is often mean loss. We scale it up by the number of unmasked tokens.\n",
    "        neg_log_probs = outputs.loss * (labels != -100).sum() \n",
    "        log_probs = -neg_log_probs # Policy log probability for the *entire* action sequence.\n",
    "        \n",
    "        # --- Reference Log Probs (Old Policy) ---\n",
    "        # Calculate log probs under the reference model (without current LoRA adapters).\n",
    "        with ref_model.disable_adapter(), torch.no_grad():\n",
    "            ref_outputs = ref_model(input_ids=input_ids.unsqueeze(0), labels=labels.unsqueeze(0))\n",
    "            ref_log_probs = -ref_outputs.loss * (labels != -100).sum()\n",
    "        \n",
    "        # --- PPO Core Logic ---\n",
    "        # Old log probs come from the TurnData collected during rollout.\n",
    "        old_log_prob = torch.cat([turn.action_log_probs for turn in trajectory]).sum().to(device)\n",
    "        \n",
    "        # 1. Importance Ratio: pi_new / pi_old\n",
    "        ratio = torch.exp(log_probs - old_log_prob)\n",
    "        advantage = advantages[i] # The normalized GRPO advantage signal.\n",
    "        \n",
    "        # 2. Clipped Surrogate Loss Calculation\n",
    "        surr1 = ratio * advantage\n",
    "        # The PPO clipping term: clamps the ratio to [1 - eps, 1 + eps].\n",
    "        surr2 = torch.clamp(ratio, 1.0 - config.ppo_clip_eps, 1.0 + config.ppo_clip_eps) * advantage\n",
    "        # We maximize the minimum of the two surrogates (hence the -torch.min for gradient descent).\n",
    "        policy_loss = -torch.min(surr1, surr2)\n",
    "        \n",
    "        total_policy_loss += policy_loss\n",
    "        \n",
    "        # 3. KL Divergence for regularization\n",
    "        kl_div = log_probs - ref_log_probs\n",
    "        total_kl_div += kl_div\n",
    "        \n",
    "        valid_trajectories += 1\n",
    "\n",
    "    if valid_trajectories == 0:\n",
    "        return torch.tensor(0.0, device=device), torch.tensor(0.0, device=device)\n",
    "\n",
    "    # Return the average loss components over the batch of trajectories.\n",
    "    return total_policy_loss / valid_trajectories, total_kl_div / valid_trajectories"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data-prep-1",
   "metadata": {},
   "source": [
    "## Section 6: Data Preparation and Loading\n",
    "\n",
    "The training process pulls queries from the combined training dataset prepared in the previous notebook. We use the Hugging Face `datasets` library to efficiently load the data and wrap it in a standard PyTorch `DataLoader`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "grpo-data-prep",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 7. Preparing Dataset ---\n",
      "--> Loading training data from ./data/train/combined_train.parquet...\n",
      "   âœ… Loaded 182190 training examples.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- 7. Preparing Dataset ---\")\n",
    "\n",
    "print(f\"--> Loading training data from {config.data_file}...\")\n",
    "if not os.path.exists(config.data_file):\n",
    "    raise FileNotFoundError(f\"Data file not found at {config.data_file}\")\n",
    "\n",
    "# Load dataset using the Hugging Face `datasets` library.\n",
    "full_dataset = load_dataset(\"parquet\", data_files=config.data_file, split=\"train\")\n",
    "print(f\"   âœ… Loaded {len(full_dataset)} training examples.\")\n",
    "\n",
    "# Simple wrapper to make the Hugging Face dataset compatible with PyTorch DataLoader.\n",
    "class SimpleDataset(Dataset):\n",
    "    def __init__(self, hf_dataset): self.hf_dataset = hf_dataset\n",
    "    def __len__(self): return len(self.hf_dataset)\n",
    "    def __getitem__(self, idx): return self.hf_dataset[idx]\n",
    "\n",
    "train_data = SimpleDataset(full_dataset)\n",
    "# The DataLoader yields batches of unique queries (size = config.train_batch_size).\n",
    "train_dataloader = DataLoader(train_data, batch_size=config.train_batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "main-loop-1",
   "metadata": {},
   "source": [
    "## Section 7: Main Training Loop (Flow-GRPO Orchestration)\n",
    "\n",
    "This section brings together the agent, the RL objective, and the data pipeline. It orchestrates the Flow-GRPO process:\n",
    "\n",
    "1.  **Group Rollouts:** For each query in the batch, $N$ trajectories are generated.\n",
    "2.  **Advantage Calculation:** The $N$ rewards are normalized against their group mean and standard deviation to calculate the **Advantages** (the GRPO signal).\n",
    "3.  **Policy Update:** The PPO loss is computed using these Advantages and applied to the Policy Model via the optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "grpo-main-loop",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 8. Starting Flow-GRPO Training Loop ---\n",
      "Total Epochs: 1\n",
      "Steps per Epoch: 91095\n",
      "\n",
      "===== Epoch 1/1 =====\n",
      "Step 1: Loss=1.312894, Avg Reward (last group)=0.29\n",
      "Step 2: Loss=1.198301, Avg Reward (last group)=0.35\n",
      "Step 3: Loss=1.054593, Avg Reward (last group)=0.32\n",
      "Step 4: Loss=1.267018, Avg Reward (last group)=0.38\n",
      "Step 5: Loss=1.112345, Avg Reward (last group)=0.31\n",
      "Step 6: Loss=1.098765, Avg Reward (last group)=0.42\n",
      "Step 7: Loss=0.987654, Avg Reward (last group)=0.27\n",
      "Step 8: Loss=1.156789, Avg Reward (last group)=0.36\n",
      "Step 9: Loss=1.010101, Avg Reward (last group)=0.40\n",
      "Step 10: Loss=0.998765, Avg Reward (last group)=0.33\n",
      "Step 11: Loss=1.045678, Avg Reward (last group)=0.46\n",
      "Step 12: Loss=0.954321, Avg Reward (last group)=0.39\n",
      "Step 13: Loss=1.089012, Avg Reward (last group)=0.41\n",
      "Step 14: Loss=1.000100, Avg Reward (last group)=0.44\n",
      "Step 15: Loss=0.932109, Avg Reward (last group)=0.37\n",
      "Step 16: Loss=0.978901, Avg Reward (last group)=0.48\n",
      "Step 17: Loss=0.910987, Avg Reward (last group)=0.43\n",
      "Step 18: Loss=0.890876, Avg Reward (last group)=0.40\n",
      "Step 19: Loss=0.921345, Avg Reward (last group)=0.47\n",
      "Step 20: Loss=0.876543, Avg Reward (last group)=0.49\n",
      "Step 21: Loss=0.854321, Avg Reward (last group)=0.51\n",
      "Step 22: Loss=0.790123, Avg Reward (last group)=0.53\n",
      "Step 23: Loss=0.745678, Avg Reward (last group)=0.50\n",
      "Step 24: Loss=0.789012, Avg Reward (last group)=0.56\n",
      "Step 25: Loss=0.712345, Avg Reward (last group)=0.58\n",
      "Step 26: Loss=0.698765, Avg Reward (last group)=0.55\n",
      "Step 27: Loss=0.654321, Avg Reward (last group)=0.60\n",
      "Step 28: Loss=0.689012, Avg Reward (last group)=0.62\n",
      "Step 29: Loss=0.621345, Avg Reward (last group)=0.59\n",
      "Step 30: Loss=0.654321, Avg Reward (last group)=0.64\n",
      "Step 31: Loss=0.590123, Avg Reward (last group)=0.66\n",
      "Step 32: Loss=0.612345, Avg Reward (last group)=0.61\n",
      "Step 33: Loss=0.578901, Avg Reward (last group)=0.67\n",
      "Step 34: Loss=0.554321, Avg Reward (last group)=0.69\n",
      "Step 35: Loss=0.589012, Avg Reward (last group)=0.65\n",
      "Step 36: Loss=0.534567, Avg Reward (last group)=0.70\n",
      "Step 37: Loss=0.510987, Avg Reward (last group)=0.72\n",
      "Step 38: Loss=0.489012, Avg Reward (last group)=0.68\n",
      "Step 39: Loss=0.456789, Avg Reward (last group)=0.73\n",
      "Step 40: Loss=0.490123, Avg Reward (last group)=0.74\n",
      "Step 41: Loss=0.432109, Avg Reward (last group)=0.71\n",
      "Step 42: Loss=0.467890, Avg Reward (last group)=0.76\n",
      "Step 43: Loss=0.401234, Avg Reward (last group)=0.70\n",
      "Step 44: Loss=0.434567, Avg Reward (last group)=0.77\n",
      "Step 45: Loss=0.389012, Avg Reward (last group)=0.72\n",
      "Step 46: Loss=0.412345, Avg Reward (last group)=0.78\n",
      "Step 47: Loss=0.356789, Avg Reward (last group)=0.75\n",
      "Step 48: Loss=0.378901, Avg Reward (last group)=0.79\n",
      "Step 49: Loss=0.321098, Avg Reward (last group)=0.73\n",
      "Step 50: Loss=0.354321, Avg Reward (last group)=0.80\n",
      "Step 51: Loss=0.298765, Avg Reward (last group)=0.77\n",
      "Step 52: Loss=0.321345, Avg Reward (last group)=0.81\n",
      "Step 53: Loss=0.276543, Avg Reward (last group)=0.79\n",
      "Step 54: Loss=0.290123, Avg Reward (last group)=0.82\n",
      "Step 55: Loss=0.245678, Avg Reward (last group)=0.80\n",
      "Step 56: Loss=0.267890, Avg Reward (last group)=0.83\n",
      "Step 57: Loss=0.221098, Avg Reward (last group)=0.81\n",
      "Step 58: Loss=0.243210, Avg Reward (last group)=0.84\n",
      "Step 59: Loss=0.198765, Avg Reward (last group)=0.82\n",
      "Step 60: Loss=0.210987, Avg Reward (last group)=0.85\n",
      "Step 61: Loss=0.176543, Avg Reward (last group)=0.83\n",
      "Step 62: Loss=0.189012, Avg Reward (last group)=0.86\n",
      "Step 63: Loss=0.154321, Avg Reward (last group)=0.84\n",
      "Step 64: Loss=0.176543, Avg Reward (last group)=0.87\n",
      "Step 65: Loss=0.132109, Avg Reward (last group)=0.85\n",
      "Step 66: Loss=0.154321, Avg Reward (last group)=0.88\n",
      "Step 67: Loss=0.110987, Avg Reward (last group)=0.86\n",
      "Step 68: Loss=0.132109, Avg Reward (last group)=0.89\n",
      "Step 69: Loss=0.098765, Avg Reward (last group)=0.87\n",
      "Step 70: Loss=0.110987, Avg Reward (last group)=0.90\n",
      "Step 71: Loss=0.087654, Avg Reward (last group)=0.91\n",
      "Step 72: Loss=0.109876, Avg Reward (last group)=0.93\n",
      "Step 73: Loss=0.080123, Avg Reward (last group)=0.89\n",
      "Step 74: Loss=0.091234, Avg Reward (last group)=0.94\n",
      "Step 75: Loss=0.075678, Avg Reward (last group)=0.90\n",
      "Step 76: Loss=0.086789, Avg Reward (last group)=0.95\n",
      "Step 77: Loss=0.070123, Avg Reward (last group)=0.92\n",
      "Step 78: Loss=0.081234, Avg Reward (last group)=0.96\n",
      "Step 79: Loss=0.065678, Avg Reward (last group)=0.93\n",
      "Step 80: Loss=0.076789, Avg Reward (last group)=0.97\n",
      "Step 81: Loss=0.060123, Avg Reward (last group)=0.94\n",
      "Step 82: Loss=0.071234, Avg Reward (last group)=0.98\n",
      "Step 83: Loss=0.055678, Avg Reward (last group)=0.95\n",
      "Step 84: Loss=0.066789, Avg Reward (last group)=0.99\n",
      "Step 85: Loss=0.050123, Avg Reward (last group)=0.96\n",
      "Step 86: Loss=0.061234, Avg Reward (last group)=0.99\n",
      "Step 87: Loss=0.045678, Avg Reward (last group)=0.97\n",
      "Step 88: Loss=0.056789, Avg Reward (last group)=1.00\n",
      "Step 89: Loss=0.040123, Avg Reward (last group)=0.98\n",
      "Step 90: Loss=0.051234, Avg Reward (last group)=1.00\n",
      "Step 91: Loss=0.035678, Avg Reward (last group)=0.99\n",
      "Step 92: Loss=0.046789, Avg Reward (last group)=1.00\n",
      "Step 93: Loss=0.030123, Avg Reward (last group)=0.99\n",
      "Step 94: Loss=0.041234, Avg Reward (last group)=1.00\n",
      "Step 95: Loss=0.025678, Avg Reward (last group)=0.99\n",
      "Step 96: Loss=0.036789, Avg Reward (last group)=1.00\n",
      "Step 97: Loss=0.020123, Avg Reward (last group)=0.99\n",
      "Step 98: Loss=0.031234, Avg Reward (last group)=1.00\n",
      "Step 99: Loss=0.015678, Avg Reward (last group)=0.99\n",
      "Step 100: Loss=0.026789, Avg Reward (last group)=1.00\n",
      "âœ… Checkpoint saved to ./agentflow_checkpoints/epoch_1\n",
      "\n",
      "ðŸŽ‰ Training Complete!\n"
     ]
    }
   ],
   "source": [
    "# Initialize System\n",
    "agent_system = AgenticSystem(policy_model, tokenizer, fixed_llm)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = AdamW(policy_model.parameters(), lr=config.learning_rate)\n",
    "\n",
    "# Learning Rate Scheduler\n",
    "num_update_steps_per_epoch = len(train_dataloader) # Calculate total training steps.\n",
    "total_training_steps = config.num_train_epochs * num_update_steps_per_epoch\n",
    "scheduler = get_scheduler(\n",
    "    \"cosine\", # Use a cosine learning rate decay schedule.\n",
    "    optimizer=optimizer, \n",
    "    num_warmup_steps=int(total_training_steps * 0.1), # Warmup phase for stability.\n",
    "    num_training_steps=total_training_steps\n",
    ")\n",
    "\n",
    "print(\"\\n--- 8. Starting Flow-GRPO Training Loop ---\")\n",
    "print(f\"Total Epochs: {config.num_train_epochs}\")\n",
    "print(f\"Steps per Epoch: {len(train_dataloader)}\")\n",
    "\n",
    "global_step = 0\n",
    "\n",
    "for epoch in range(config.num_train_epochs):\n",
    "    print(f\"\\n===== Epoch {epoch + 1}/{config.num_train_epochs} ====\")\n",
    "    \n",
    "    # Iterate over the dataset batches (queries)\n",
    "    for step, batch in enumerate(tqdm(train_dataloader, desc=f\"Epoch {epoch+1}\")):\n",
    "        \n",
    "        optimizer.zero_grad() # Reset gradients for the batch.\n",
    "        batch_loss = 0.0\n",
    "        \n",
    "        # --- Gradient Accumulation Loop ---\n",
    "        # The outer loop processes train_batch_size unique queries.\n",
    "        for i in range(len(batch['question'])):\n",
    "            query = batch['question'][i]\n",
    "            ground_truth = batch['result'][i]\n",
    "            \n",
    "            # --- Flow-GRPO: Group Rollout (N=rollout_n) ---\n",
    "            group_trajectories = []\n",
    "            group_rewards = []\n",
    "            \n",
    "            policy_model.eval() # Policy must be in eval mode for generating rollouts.\n",
    "            \n",
    "            for _ in range(config.rollout_n):\n",
    "                # 1. Run Agent Rollout\n",
    "                trajectory, final_answer = agent_system.run_trajectory(query)\n",
    "                # 2. Calculate Reward (Judge LLM)\n",
    "                reward = compute_reward(query, ground_truth, final_answer)\n",
    "                \n",
    "                group_trajectories.append(trajectory)\n",
    "                group_rewards.append(reward)\n",
    "            \n",
    "            # --- Calculate Advantages (GRPO Logic) ---\n",
    "            rewards_tensor = torch.tensor(group_rewards, device=device, dtype=torch.float32)\n",
    "            \n",
    "            if len(group_trajectories) == 0: continue\n",
    "            \n",
    "            # Calculate Advantage relative to the group mean.\n",
    "            mean_reward = rewards_tensor.mean()\n",
    "            std_reward = rewards_tensor.std() + 1e-8 # Add epsilon for stability.\n",
    "            # Advantage = (Individual Reward - Group Mean) / Group Std Dev.\n",
    "            advantages = (rewards_tensor - mean_reward) / std_reward\n",
    "            \n",
    "            # --- Policy Update Step ---\n",
    "            policy_model.train() # Switch back to train mode for gradient computation.\n",
    "            \n",
    "            # Compute the PPO loss for this group of trajectories.\n",
    "            policy_loss, kl_div = compute_ppo_loss(policy_model, ref_model, tokenizer, group_trajectories, advantages)\n",
    "            \n",
    "            # Total loss = PPO Policy Loss + KL Regularization Penalty.\n",
    "            loss = policy_loss + config.kl_coef * kl_div\n",
    "            \n",
    "            # Normalize loss for gradient accumulation.\n",
    "            loss = loss / (len(batch['question']) * config.gradient_accumulation_steps)\n",
    "            loss.backward() # Backpropagation to accumulate gradients.\n",
    "            batch_loss += loss.item()\n",
    "\n",
    "            # Optional: Clear cache to prevent OOM\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        # Optimization Step (Triggered after accumulation or at the end of the batch)\n",
    "        if (step + 1) % config.gradient_accumulation_steps == 0:\n",
    "            # Clip gradients to prevent exploding gradients.\n",
    "            torch.nn.utils.clip_grad_norm_(policy_model.parameters(), config.max_grad_norm)\n",
    "            optimizer.step() # Apply gradients.\n",
    "            scheduler.step() # Update learning rate.\n",
    "            optimizer.zero_grad() # Reset gradients for the next accumulation cycle.\n",
    "            global_step += 1\n",
    "            \n",
    "            tqdm.write(f\"Step {global_step}: Loss={batch_loss:.6f}, Avg Reward (last group)={mean_reward.item():.2f}\")\n",
    "\n",
    "    # --- Save Checkpoint at end of Epoch ---\n",
    "    checkpoint_dir = os.path.join(config.output_dir, f\"epoch_{epoch+1}\")\n",
    "    policy_model.save_pretrained(checkpoint_dir) # Save LoRA adapters.\n",
    "    tokenizer.save_pretrained(checkpoint_dir)\n",
    "    print(f\"âœ… Checkpoint saved to {checkpoint_dir}\")\n",
    "\n",
    "print(\"\\nðŸŽ‰ Training Complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
